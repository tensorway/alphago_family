{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd95df06ad6614d8ea31ce2c635a29972",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4683776676063258"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from gym_tictactoe.env import TicTacToeEnv, agent_by_mark, next_mark\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from myrl.buffers import ReplayBuffer\n",
    "from myrl.utils import ExperimentWriter\n",
    "import copy\n",
    "# from myrl.value_functions import \n",
    "\n",
    "env = TicTacToeEnv()\n",
    "obs = env.reset()\n",
    "\n",
    "import random\n",
    "import math\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, obs, render=False):\n",
    "        return env.action_space.sample()\n",
    "    def rollout(self, obs, model, render=False):\n",
    "        d = False\n",
    "        rsum = 0\n",
    "        while not d:\n",
    "            obs, r, d, _ = model.step(obs, self.act(obs))\n",
    "            rsum += r\n",
    "        return rsum\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def step(self, obs, action):\n",
    "        self._set_env(obs)\n",
    "        return self.env.step(action)\n",
    "    def available_actions(self, obs):\n",
    "        self._set_env(obs)\n",
    "        return self.env.available_actions()\n",
    "    def not_available_actions(self, obs):\n",
    "        return self._list_cut(list(range(self.get_num_actions())), self.available_actions(obs))\n",
    "    def _list_cut(self, l1, l2):\n",
    "        toret = []\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret\n",
    "    def _set_env(self, obs):\n",
    "\n",
    "        self.env.board = list(obs[0])\n",
    "        self.env.mark  = obs[1] \n",
    "        self.done = False\n",
    "    def get_num_actions(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "class TreePolicy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def act(self, obs, available_actions):\n",
    "        import random\n",
    "        return random.choice(available_actions)\n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        return [1/len(available_actions) for i in range(len(available_actions))]\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, net_arch, middle_activation=F.relu, last_activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.middle_activation = middle_activation\n",
    "        self.last_activation = last_activation\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = self.middle_activation(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        h = self.last_activation(h)\n",
    "        return h\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, net_arch, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = torch.tensor(h, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = F.relu(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        return h\n",
    "\n",
    "class NNTreePolicy(nn.Module):\n",
    "    def __init__(self, net_arch, backbone, temperature=1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.temperature = temperature\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, x, not_available_actions=None):\n",
    "        h = torch.tensor(x, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = F.tanh(lay(h))\n",
    "        h = self.layers[-1](h)/self.temperature\n",
    "        if not_available_actions is not None and len(not_available_actions)>0:\n",
    "            not_available_actions = torch.tensor(not_available_actions)\n",
    "            h[0, not_available_actions] = float('-inf')\n",
    "        h = torch.softmax(h, dim=1)\n",
    "        return h\n",
    "    def act(self, obs, not_available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs, not_available_actions=not_available_actions)\n",
    "        action = np.random.choice(range(len(h[0])), p=h.detach().squeeze(0).numpy())\n",
    "        return action      \n",
    "    def get_action_probs(self, obs, available_actions=None):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs)\n",
    "        return h.tolist()[0]\n",
    "    def obs2testorobs(self, obs):\n",
    "        l2 = [1] if obs[1]=='O' else [-1]\n",
    "        obs = torch.tensor([list(obs[0])+l2])\n",
    "        obs[obs==2] = -1\n",
    "        return obs\n",
    "\n",
    "class CNNTreePolicy(NNTreePolicy):\n",
    "    def __init__(self, net_arch, backbone, temperature=1):\n",
    "        pass\n",
    "\n",
    "rollout_policy = RolloutPolicy(env)\n",
    "model = Model(TicTacToeEnv())\n",
    "backbone = Backbone([10, 64])\n",
    "value_function = ValueFunction([64, 32, 1], backbone=backbone)\n",
    "tree_policy = NNTreePolicy([64, 64, 9], backbone=backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_winrate(totest, bench, env, n_games=100):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    rsum = 0\n",
    "    for igame in range(n_games):\n",
    "        done, reward = False, 0\n",
    "        obs = env.reset()\n",
    "        curr_policy = totest if igame<=n_games//2 else bench\n",
    "        rew2count = 1 if igame<=n_games//2 else -1\n",
    "        while not done:\n",
    "            action = curr_policy.act(obs, model.not_available_actions(obs))\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            rsum += r\n",
    "            curr_policy = totest if curr_policy==bench else bench\n",
    "            wins += 1 if r==rew2count else 0\n",
    "        draws += 1 if r==0 else 0\n",
    "    winrate = wins/n_games\n",
    "    drawrate = draws/n_games\n",
    "    return winrate, drawrate, (1-winrate-drawrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.77, 0.032, 0.19799999999999998)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "eval_winrate(tree_policy, rollout_policy, TicTacToeEnv(), n_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_policy.temperature = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_sign(tensor_obs, idx):\n",
    "    signs = [1, -1]\n",
    "    return tensor_obs*signs[idx]\n",
    "def flip_board(tensor_obs, idx):\n",
    "    ops = [[], [0], [1], [0, 1]]\n",
    "    return torch.flip(tensor_obs, ops[idx])\n",
    "def rotate_board(tensor_obs, idx):\n",
    "    none = lambda x:x\n",
    "    rot  = lambda x:torch.rot90(x, 1, [0, 1])\n",
    "    ops = [none, rot]\n",
    "    return ops[idx](tensor_obs)\n",
    "def symetric_add2rbuff(rbuff, list_with_boards, rew_sign):\n",
    "    added_boards = set()\n",
    "    for tensor_obs, monte_probs, rew_sign in list_with_boards:\n",
    "        for sign in range(2):\n",
    "            for flip in range(4):\n",
    "                for rotate in range(2):\n",
    "                    side_len = 3#int(math.sqrt(tensor_obs.shape[0]*tensor_obs.shape[1]))\n",
    "                    # print(side_len)\n",
    "                    board_obs = tensor_obs.view(1, -1)\n",
    "                    board_obs, player = board_obs[:, :-1], board_obs[:, -1].unsqueeze(-1)\n",
    "                    board_obs = board_obs.reshape(side_len, side_len)\n",
    "                    sign_boardt = flip_sign(board_obs, sign)\n",
    "                    player = flip_sign(player, sign)\n",
    "                    flip_boardt = flip_board(sign_boardt, flip)\n",
    "                    board = rotate_board(flip_boardt, rotate)\n",
    "                    board = board.reshape(1, -1)\n",
    "                    board = torch.cat((board, player), dim=-1)\n",
    "                    board = board.view(1, 1, -1)\n",
    "\n",
    "\n",
    "                    board_monte = monte_probs.reshape(side_len, side_len)\n",
    "                    # sign_montet = flip_sign(board_monte, sign)\n",
    "                    flip_montet = flip_board(board_monte, flip)\n",
    "                    monte = rotate_board(flip_montet, rotate)\n",
    "                    monte = monte.reshape(1, 1, -1)\n",
    "\n",
    "                    if board not in added_boards:\n",
    "                        added_boards.add(board)\n",
    "                        raw_reward = torch.tensor([[[r]]]).float()\n",
    "                        flip_rew_sign = 1 if sign==0 else -1\n",
    "                        reward_now = raw_reward*flip_rew_sign*rew_sign\n",
    "                        rbuff.add(board, monte, reward_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(root_node, policy, model, cpucb=150):\n",
    "    available_actions = model.available_actions(root_node.obs)\n",
    "    probs = policy.get_action_probs(root_node.obs)\n",
    "    minscore, a_minscore = float('inf'), -1\n",
    "    for action in available_actions:\n",
    "        if action in root_node.action2child.keys():\n",
    "            child = root_node.action2child[action]\n",
    "            q =  child.get_q()\n",
    "            u = -probs[action]/(1+child.n)\n",
    "        else:\n",
    "            q = 0\n",
    "            u = float('-inf')\n",
    "        score = q + cpucb*u\n",
    "        if score < minscore:\n",
    "            minscore = score\n",
    "            a_minscore = action\n",
    "    return a_minscore\n",
    "\n",
    "def MCTS2(root_node, max_depth, n_times, policy, model, cpucb=150):\n",
    "    current_node  = root_node\n",
    "    current_depth = 0\n",
    "    n_times_done  = 0\n",
    "\n",
    "    while n_times_done != n_times:\n",
    "        if current_depth == max_depth or current_node.done:\n",
    "            reward = current_node.rollout(rollout_policy, model)\n",
    "            if not current_node.done:\n",
    "                reward2 = value_function(tree_policy.obs2testorobs(current_node.obs)).item()\n",
    "                print(reward, reward2, \"reward \"*3)\n",
    "            current_node.backpropagate(reward, gamma=0.99)\n",
    "            current_node = root_node\n",
    "            n_times_done += 1\n",
    "            current_depth = 0\n",
    "            model.env.done = False\n",
    "        else:\n",
    "            action = UCB(current_node, policy, model, cpucb)\n",
    "            if action in current_node.action2child:\n",
    "                current_node = current_node.action2child[action]\n",
    "            else:\n",
    "                current_node = current_node.create_child(Node2, model, action)\n",
    "            current_depth += 1\n",
    "\n",
    "    visits = []\n",
    "    for a in range(model.get_num_actions()):\n",
    "        if a in root_node.action2child:\n",
    "            visits.append(root_node.action2child[a].get_q())\n",
    "        else:\n",
    "            visits.append(float('inf'))\n",
    "    return visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node2:\n",
    "    def __init__(self, obs, reward, change_child_rew_sign=True, reward_sign=1, done=False, parent=None):\n",
    "        self.n = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.parent = parent\n",
    "        self.action2child = {}\n",
    "        self.obs = obs\n",
    "        self.done = done\n",
    "        self.reward = reward\n",
    "        self.reward_sign = reward_sign\n",
    "        self.change_child_rew_sign = change_child_rew_sign\n",
    "        \n",
    "    def get_q(self):\n",
    "        return self.cumulative_reward/(self.n)\n",
    "\n",
    "    def backpropagate(self, r, gamma=1):\n",
    "        self.n += 1\n",
    "        self.cumulative_reward += r*gamma*self.reward_sign\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(r, gamma=gamma)\n",
    "\n",
    "    def create_child(self, ChildType, model, action):\n",
    "        obs, reward, done, info = model.step(self.obs, action)\n",
    "        reward_sign= -self.reward_sign if self.change_child_rew_sign else self.reward_sign\n",
    "        child = ChildType(obs, reward, done=done, reward_sign=reward_sign, parent=self, change_child_rew_sign=self.change_child_rew_sign)\n",
    "        self.action2child[action] = child\n",
    "        return child  \n",
    "\n",
    "    def rollout(self, rollout_policy, model, render=False):\n",
    "        if self.done:\n",
    "            return self.reward\n",
    "        return rollout_policy.rollout(self.obs, model, render=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "69948387146 reward reward reward \n",
      "-1 -0.04248036444187164 reward reward reward \n",
      "1 -0.04248036444187164 reward reward reward \n",
      "-1 0.1199137270450592 reward reward reward \n",
      "0 0.1199137270450592 reward reward reward \n",
      "-1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "-1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "1 0.1199137270450592 reward reward reward \n",
      "move probs=  tensor([[1.3170e-07, 9.9725e-01, 0.0000e+00],\n",
      "        [1.2063e-04, 0.0000e+00, 0.0000e+00],\n",
      "        [1.3170e-07, 2.6248e-03, 1.3170e-07]], dtype=torch.float64)\n",
      "1 3 X\n",
      "   | |X\n",
      "  -----\n",
      "  X|O|O\n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "move probs=  tensor([[1.0627e-04, 1.2586e-09, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.9995e-01, 1.2586e-09, 4.9995e-01]], dtype=torch.float64)\n",
      "6 0 O\n",
      "  O| |X\n",
      "  -----\n",
      "  X|O|O\n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "-1 -0.14199160039424896 reward reward reward \n",
      "1 0.31155940890312195 reward reward reward \n",
      "-1 0.03571861982345581 reward reward reward \n",
      "0 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "0 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "0 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 -0.14199160039424896 reward reward reward \n",
      "1 0.03571861982345581 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "1 0.19639092683792114 reward reward reward \n",
      "-1 0.19639092683792114 reward reward reward \n",
      "move probs=  tensor([[0.0000e+00, 4.9286e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.4729e-05, 4.9286e-01, 1.4254e-02]], dtype=torch.float64)\n",
      "1 8 X\n",
      "  O| |X\n",
      "  -----\n",
      "  X|O|O\n",
      "  -----\n",
      "   | |X\n",
      "\n",
      " \n",
      " \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "move probs=  tensor([[0.0000e+00, 1.1025e-05, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.9998e-01, 1.1025e-05, 0.0000e+00]], dtype=torch.float64)\n",
      "6 6 O\n",
      "  O| |X\n",
      "  -----\n",
      "  X|O|O\n",
      "  -----\n",
      "  O| |X\n",
      "\n",
      " \n",
      " \n",
      "-1 -0.20908930897712708 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "0 -0.20908930897712708 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "1 -0.20908930897712708 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "1 -0.023217350244522095 reward reward reward \n",
      "0 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "-1 -0.023217350244522095 reward reward reward \n",
      "move probs=  tensor([[0.0000, 0.0865, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9135, 0.0000]], dtype=torch.float64)\n",
      "7 1 X\n",
      "  O|X|X\n",
      "  -----\n",
      "  X|O|O\n",
      "  -----\n",
      "  O| |X\n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64)\n",
      "7 7 O\n",
      "  O|X|X\n",
      "  -----\n",
      "  X|O|O\n",
      "  -----\n",
      "  O|O|X\n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "\n",
    "random_policy = TreePolicy()\n",
    "\n",
    "while not done:\n",
    "    rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "    root = Node2(obs, 0, reward_sign=rew_sign)\n",
    "    # print(tree_policy(tree_policy.obs2testorobs(obs)))\n",
    "    # print(tree_policy(tree_policy.obs2testorobs(obs), model.not_available_actions(obs)))\n",
    "    # print(tree_policy.act(obs, model.not_available_actions(obs)))\n",
    "    besta = tree_policy.act(obs, model.not_available_actions(obs))\n",
    "    dic = MCTS2(root, 1, 100, tree_policy, model, 100)\n",
    "    dic = np.array(dic)\n",
    "    tdic = torch.tensor([-dic])\n",
    "    print(\"move probs= \", torch.softmax(tdic*10, dim=1).view(3, 3))\n",
    "    move = np.argmin(dic)\n",
    "    print(move, besta, test_env.mark)\n",
    "    # print(dic.reshape(3, 3))\n",
    "    obs, r, done, _ = test_env.step(besta)\n",
    "    test_env.render()\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = copy.deepcopy(tree_policy)\n",
    "wll = ExperimentWriter('tb/alpha_tictacte_zero_symetric_probs_withARENA____')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_policy.temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65127023 1.434959888458252\n",
      "509 winrate= 0.77 0.06\n",
      "loss= 3.690540838937007 2.2853763587276563 1.4051644802093506\n",
      "510 winrate= 0.81 0.01\n",
      "loss= 3.478757577403076 2.0890408329801633 1.3897167444229126\n",
      "ARENA!!!  0.32 0.25 0.42999999999999994\n",
      "511 winrate= 0.77 0.06\n",
      "loss= 3.6741971089923546 2.247821362265365 1.4263757467269897\n",
      "512 winrate= 0.76 0.05\n",
      "loss= 3.5669531080458636 2.1460446569655414 1.4209084510803223\n",
      "513 winrate= 0.75 0.06\n",
      "loss= 3.7000273124207257 2.267028178357005 1.4329991340637207\n",
      "514 winrate= 0.82 0.01\n",
      "loss= 3.5224562136011057 2.1017477957086497 1.420708417892456\n",
      "515 winrate= 0.81 0.02\n",
      "loss= 3.4167942751836637 2.0126036156606535 1.4041906595230103\n",
      "516 winrate= 0.77 0.06\n",
      "loss= 3.809126016724957 2.370811102021588 1.4383149147033691\n",
      "517 winrate= 0.78 0.05\n",
      "loss= 3.562494380377054 2.112750990293741 1.449743390083313\n",
      "ARENA!!!  0.34 0.26 0.3999999999999999\n",
      "518 winrate= 0.74 0.06\n",
      "loss= 3.5895875863307234 2.1835323743098494 1.406055212020874\n",
      "519 winrate= 0.79 0.04\n",
      "loss= 3.501734502389283 2.087795384003968 1.413939118385315\n",
      "520 winrate= 0.72 0.06\n",
      "loss= 3.545934710215249 2.08789220781199 1.4580425024032593\n",
      "521 winrate= 0.79 0.03\n",
      "loss= 3.5736819590506115 2.2025319422659435 1.371150016784668\n",
      "522 winrate= 0.78 0.06\n",
      "loss= 3.5967920528323223 2.1831747995287945 1.4136172533035278\n",
      "523 winrate= 0.76 0.04\n",
      "loss= 3.598742704878165 2.156514029989554 1.4422286748886108\n",
      "524 winrate= 0.8 0.05\n",
      "loss= 3.492240656077809 2.102233279406972 1.3900073766708374\n",
      "ARENA!!!  0.37 0.18 0.45\n",
      "525 winrate= 0.71 0.06\n",
      "loss= 3.761862541381522 2.3277490388790807 1.4341135025024414\n",
      "526 winrate= 0.76 0.02\n",
      "loss= 3.4972636596422113 2.099709977314941 1.3975536823272705\n",
      "527 winrate= 0.79 0.02\n",
      "loss= 3.6816939554609798 2.2246895275511287 1.457004427909851\n",
      "528 winrate= 0.79 0.02\n",
      "loss= 3.5158193632206847 2.1202065273365904 1.3956128358840942\n",
      "529 winrate= 0.79 0.04\n",
      "loss= 3.5861174266307225 2.1869873206584325 1.39913010597229\n",
      "530 winrate= 0.84 0.01\n",
      "loss= 3.546116465006336 2.1189966138939704 1.4271198511123657\n",
      "531 winrate= 0.76 0.04\n",
      "loss= 3.5837237220449896 2.168737111632392 1.4149866104125977\n",
      "ARENA!!!  0.33 0.33 0.3399999999999999\n",
      "532 winrate= 0.79 0.04\n",
      "loss= 3.5618559574007778 2.1636437629580287 1.398212194442749\n",
      "533 winrate= 0.74 0.04\n",
      "loss= 3.35875730192853 1.9666543451566791 1.3921029567718506\n",
      "534 winrate= 0.77 0.02\n",
      "loss= 3.6181235184238485 2.2190862764881185 1.39903724193573\n",
      "535 winrate= 0.74 0.05\n",
      "loss= 3.699606801198988 2.2573592912426284 1.4422475099563599\n",
      "536 winrate= 0.76 0.0\n",
      "loss= 3.5667856452822324 2.141544294154608 1.4252413511276245\n",
      "537 winrate= 0.77 0.01\n",
      "loss= 3.4392449084776437 2.0423838321226633 1.3968610763549805\n",
      "538 winrate= 0.8 0.02\n",
      "loss= 3.65429877710143 2.235030765054616 1.419268012046814\n",
      "ARENA!!!  0.34 0.26 0.3999999999999999\n",
      "539 winrate= 0.73 0.08\n",
      "loss= 3.5350682768889508 2.1120368275710186 1.4230314493179321\n",
      "540 winrate= 0.79 0.02\n",
      "loss= 3.7648822619936095 2.3383128001710998 1.4265694618225098\n",
      "541 winrate= 0.67 0.05\n",
      "loss= 3.7456128530716026 2.289376013777169 1.4562368392944336\n",
      "542 winrate= 0.82 0.01\n",
      "loss= 3.567235847128919 2.1476487118139778 1.4195871353149414\n",
      "543 winrate= 0.8 0.05\n",
      "loss= 3.561366878812797 2.1505839455776283 1.4107829332351685\n",
      "544 winrate= 0.78 0.0\n",
      "loss= 3.4802019031153346 2.042375984917703 1.4378259181976318\n",
      "545 winrate= 0.78 0.05\n",
      "loss= 3.552579417310929 2.1421325350630043 1.4104468822479248\n",
      "ARENA!!!  0.4 0.22 0.38\n",
      "upgrade 0.4 0.22 0.38\n",
      "winrate against random  0.769 0.024\n",
      "546 winrate= 0.81 0.04\n",
      "loss= 3.397243791289248 1.9830656582783834 1.4141781330108643\n",
      "547 winrate= 0.81 0.02\n",
      "loss= 3.7197143710549616 2.2970249331887507 1.422689437866211\n",
      "548 winrate= 0.78 0.02\n",
      "loss= 3.555969947414705 2.1508598266413896 1.4051101207733154\n",
      "549 winrate= 0.75 0.04\n",
      "loss= 3.6061459333475114 2.179614856916237 1.4265310764312744\n",
      "550 winrate= 0.81 0.04\n",
      "loss= 3.624775942966287 2.2554696170538113 1.3693063259124756\n",
      "551 winrate= 0.76 0.04\n",
      "loss= 3.672670396122441 2.262795003208623 1.4098753929138184\n",
      "552 winrate= 0.77 0.04\n",
      "loss= 3.7697637233075483 2.3122728022870405 1.4574909210205078\n",
      "ARENA!!!  0.34 0.26 0.3999999999999999\n",
      "553 winrate= 0.82 0.01\n",
      "loss= 3.5352506125424283 2.155083962724008 1.3801666498184204\n",
      "554 winrate= 0.71 0.03\n",
      "loss= 3.6485926017748986 2.1929009065615808 1.4556916952133179\n",
      "555 winrate= 0.78 0.04\n",
      "loss= 3.531193915670445 2.133805695837071 1.397388219833374\n",
      "556 winrate= 0.74 0.02\n",
      "loss= 3.515334453560604 2.0987573528068197 1.4165771007537842\n",
      "557 winrate= 0.8 0.03\n",
      "loss= 3.563277608186023 2.1443003475964235 1.4189772605895996\n",
      "558 winrate= 0.84 0.05\n",
      "loss= 3.791303047479272 2.382381924928308 1.4089211225509644\n",
      "559 winrate= 0.72 0.02\n",
      "loss= 3.764643380929583 2.3174516656891044 1.4471917152404785\n",
      "ARENA!!!  0.36 0.24 0.4\n",
      "560 winrate= 0.8 0.04\n",
      "loss= 3.5839654928907723 2.1225261217817635 1.4614393711090088\n",
      "561 winrate= 0.79 0.05\n",
      "loss= 3.6799774459223915 2.2565866998057533 1.4233907461166382\n",
      "562 winrate= 0.76 0.05\n",
      "loss= 3.6858093302885075 2.2385869782606145 1.447222352027893\n",
      "563 winrate= 0.78 0.02\n",
      "loss= 3.6096669449383887 2.17643380303928 1.4332331418991089\n",
      "564 winrate= 0.87 0.02\n",
      "loss= 3.571099763189656 2.145536189352376 1.4255635738372803\n",
      "565 winrate= 0.79 0.04\n",
      "loss= 3.4598566983698955 2.1023655627726665 1.357491135597229\n",
      "566 winrate= 0.82 0.06\n",
      "loss= 3.6985187722371164 2.251308221928889 1.4472105503082275\n",
      "ARENA!!!  0.31 0.29 0.39999999999999997\n",
      "567 winrate= 0.78 0.05\n",
      "loss= 3.7871264528322084 2.3149533580827577 1.4721730947494507\n",
      "568 winrate= 0.75 0.05\n",
      "loss= 3.633691698773926 2.216681510671204 1.4170101881027222\n",
      "569 winrate= 0.74 0.02\n",
      "loss= 3.577747319923026 2.168308829055411 1.4094384908676147\n",
      "570 winrate= 0.73 0.02\n",
      "loss= 3.6302136749213667 2.21367436351237 1.4165393114089966\n",
      "571 winrate= 0.82 0.05\n",
      "loss= 3.4403956047610365 2.0464645735339246 1.3939310312271118\n",
      "572 winrate= 0.79 0.05\n",
      "loss= 3.65535663946464 2.2657084403832193 1.389648199081421\n",
      "573 winrate= 0.82 0.05\n",
      "loss= 3.654627802368251 2.2075825953446304 1.4470452070236206\n",
      "ARENA!!!  0.41 0.29 0.3000000000000001\n",
      "upgrade 0.41 0.29 0.3000000000000001\n",
      "winrate against random  0.781 0.033\n",
      "574 winrate= 0.77 0.06\n",
      "loss= 3.448315277554271 2.071560636021373 1.376754641532898\n",
      "575 winrate= 0.77 0.03\n",
      "loss= 3.3783703046284677 1.9801416593037604 1.398228645324707\n",
      "576 winrate= 0.82 0.03\n",
      "loss= 3.4573149472304747 2.036842563731329 1.4204723834991455\n",
      "577 winrate= 0.73 0.02\n",
      "loss= 3.5237250799588358 2.0962203020505106 1.4275047779083252\n",
      "578 winrate= 0.76 0.04\n",
      "loss= 3.65822958718942 2.2356300331275305 1.4225995540618896\n",
      "579 winrate= 0.75 0.05\n",
      "loss= 3.4903526872592496 2.0915232271152067 1.398829460144043\n",
      "580 winrate= 0.81 0.05\n",
      "loss= 3.7100491836201828 2.2693738057744186 1.4406753778457642\n",
      "ARENA!!!  0.34 0.34 0.3199999999999999\n",
      "upgrade 0.34 0.34 0.3199999999999999\n",
      "winrate against random  0.786 0.038\n",
      "581 winrate= 0.74 0.06\n",
      "loss= 3.45391204631069 2.013035443184164 1.4408766031265259\n",
      "582 winrate= 0.78 0.05\n",
      "loss= 3.5289579177989117 2.1195086980952373 1.4094492197036743\n",
      "583 winrate= 0.77 0.02\n",
      "loss= 3.8147125937221382 2.3306434847591255 1.4840691089630127\n",
      "584 winrate= 0.8 0.02\n",
      "loss= 3.2398861635589187 1.8509744394683427 1.3889117240905762\n",
      "585 winrate= 0.74 0.03\n",
      "loss= 3.7429338665970366 2.2757947417267363 1.4671391248703003\n",
      "586 winrate= 0.83 0.02\n",
      "loss= 3.5102984075904944 2.094244635617838 1.4160537719726562\n",
      "587 winrate= 0.75 0.01\n",
      "loss= 3.6628975761830813 2.2084993017613894 1.454398274421692\n",
      "ARENA!!!  0.4 0.31 0.29\n",
      "upgrade 0.4 0.31 0.29\n",
      "winrate against random  0.784 0.029\n",
      "588 winrate= 0.77 0.03\n",
      "loss= 3.653162100670951 2.1935938385707314 1.4595682621002197\n",
      "589 winrate= 0.73 0.03\n",
      "loss= 3.6011926151756124 2.1816655613426046 1.4195270538330078\n",
      "590 winrate= 0.78 0.04\n",
      "loss= 3.523841414226338 2.0666497085226028 1.4571917057037354\n",
      "591 winrate= 0.78 0.05\n",
      "loss= 3.539192036350242 2.1404395854072487 1.3987524509429932\n",
      "592 winrate= 0.74 0.05\n",
      "loss= 3.6253342501600647 2.2053383461912537 1.419995903968811\n",
      "593 winrate= 0.83 0.03\n",
      "loss= 3.6339044402390814 2.184444291427741 1.4494601488113403\n",
      "594 winrate= 0.77 0.05\n",
      "loss= 3.617574791427695 2.2025707764588227 1.415004014968872\n",
      "ARENA!!!  0.32 0.27 0.4099999999999999\n",
      "595 winrate= 0.82 0.06\n",
      "loss= 3.496274032728744 2.059749760763717 1.4365242719650269\n",
      "596 winrate= 0.74 0.05\n",
      "loss= 3.7349831924932158 2.295934186984793 1.4390490055084229\n",
      "597 winrate= 0.76 0.03\n",
      "loss= 3.703672984497306 2.267519453422782 1.436153531074524\n",
      "598 winrate= 0.81 0.04\n",
      "loss= 3.4640623754068356 2.0621999925180416 1.401862382888794\n",
      "599 winrate= 0.81 0.03\n",
      "loss= 3.6414886407030242 2.2188646487367767 1.4226239919662476\n",
      "600 winrate= 0.79 0.07\n",
      "loss= 3.6923208132815812 2.260577549083278 1.4317432641983032\n",
      "601 winrate= 0.71 0.04\n",
      "loss= 3.7042609491135634 2.2620846071030654 1.442176342010498\n",
      "ARENA!!!  0.3 0.2 0.49999999999999994\n",
      "602 winrate= 0.77 0.07\n",
      "loss= 3.477160941937788 2.0750041121038034 1.4021568298339844\n",
      "603 winrate= 0.78 0.02\n",
      "loss= 3.443168454420728 2.0458437731386847 1.3973246812820435\n",
      "604 winrate= 0.82 0.03\n",
      "loss= 3.5787594483947682 2.164491908416741 1.4142675399780273\n",
      "605 winrate= 0.79 0.03\n",
      "loss= 3.5947803148285655 2.1543642171875743 1.4404160976409912\n",
      "606 winrate= 0.82 0.03\n",
      "loss= 3.5474994869522 2.119706460981741 1.427793025970459\n",
      "607 winrate= 0.9 0.02\n",
      "loss= 3.47229650433331 2.026967791869931 1.445328712463379\n",
      "608 winrate= 0.8 0.04\n",
      "loss= 3.577409003227144 2.1630755405119952 1.414333462715149\n",
      "ARENA!!!  0.31 0.25 0.43999999999999995\n",
      "609 winrate= 0.73 0.04\n",
      "loss= 3.6173571166917027 2.2142590103074253 1.4030981063842773\n",
      "610 winrate= 0.66 0.03\n",
      "loss= 3.584166860685622 2.1554817916014546 1.4286850690841675\n",
      "611 winrate= 0.78 0.11\n",
      "loss= 3.5002573006150666 2.1137179605958405 1.386539340019226\n",
      "612 winrate= 0.72 0.04\n",
      "loss= 3.496557572744002 2.1268927844978474 1.3696647882461548\n",
      "613 winrate= 0.78 0.04\n",
      "loss= 3.5728152062901137 2.1490547921665786 1.4237604141235352\n",
      "614 winrate= 0.79 0.01\n",
      "loss= 3.5734273823520954 2.1567810210010823 1.4166463613510132\n",
      "615 winrate= 0.79 0.06\n",
      "loss= 3.5392601254551326 2.1374875071613704 1.4017726182937622\n",
      "ARENA!!!  0.36 0.2 0.44\n",
      "616 winrate= 0.78 0.0\n",
      "loss= 3.7326918015767623 2.321256555109172 1.4114352464675903\n",
      "617 winrate= 0.77 0.01\n",
      "loss= 3.431649443211928 2.0160386529592427 1.4156107902526855\n",
      "618 winrate= 0.8 0.03\n",
      "loss= 3.539157771008645 2.1420885552341087 1.3970692157745361\n",
      "619 winrate= 0.82 0.03\n",
      "loss= 3.558575857348437 2.1683212406110712 1.3902546167373657\n",
      "620 winrate= 0.76 0.04\n",
      "loss= 3.55416382805203 2.142286672748075 1.411877155303955\n",
      "621 winrate= 0.78 0.05\n",
      "loss= 3.5224614076458085 2.1084506444774735 1.414010763168335\n",
      "622 winrate= 0.79 0.04\n",
      "loss= 3.6301113532765736 2.2092864679082265 1.4208248853683472\n",
      "ARENA!!!  0.46 0.21 0.33000000000000007\n",
      "upgrade 0.46 0.21 0.33000000000000007\n",
      "winrate against random  0.764 0.037\n",
      "623 winrate= 0.76 0.07\n",
      "loss= 3.485440185933399 2.0588396104863165 1.4266005754470825\n",
      "624 winrate= 0.82 0.03\n",
      "loss= 3.734492365807321 2.320332114189889 1.4141602516174316\n",
      "625 winrate= 0.81 0.07\n",
      "loss= 3.5557288700023304 2.166158037749828 1.3895708322525024\n",
      "626 winrate= 0.82 0.03\n",
      "loss= 3.606566394123462 2.1892338641017943 1.4173325300216675\n",
      "627 winrate= 0.79 0.05\n",
      "loss= 3.63117477294271 2.2222053706294775 1.4089694023132324\n",
      "628 winrate= 0.72 0.06\n",
      "loss= 3.7389118385273448 2.3097684097248545 1.4291434288024902\n",
      "629 winrate= 0.76 0.05\n",
      "loss= 3.64967235192257 2.231274448480187 1.4183979034423828\n",
      "ARENA!!!  0.3 0.3 0.39999999999999997\n",
      "630 winrate= 0.77 0.01\n",
      "loss= 3.5934096653126155 2.2023106891773616 1.391098976135254\n",
      "631 winrate= 0.86 0.02\n",
      "loss= 3.606703007277802 2.1798152566521916 1.4268877506256104\n",
      "632 winrate= 0.72 0.05\n",
      "loss= 3.509547744019185 2.1037850715948685 1.4057626724243164\n",
      "633 winrate= 0.85 0.02\n",
      "loss= 3.5544603616668526 2.1353956014587228 1.4190647602081299\n",
      "634 winrate= 0.79 0.04\n",
      "loss= 3.4962035060269674 2.089022958312734 1.4071805477142334\n",
      "635 winrate= 0.82 0.06\n",
      "loss= 3.471485162339013 2.067394161782067 1.4040910005569458\n",
      "636 winrate= 0.74 0.08\n",
      "loss= 3.5710049524724594 2.1642809763372055 1.406723976135254\n",
      "ARENA!!!  0.34 0.28 0.3799999999999999\n",
      "637 winrate= 0.76 0.07\n",
      "loss= 3.4451752997387337 2.0045155383099007 1.440659761428833\n",
      "638 winrate= 0.75 0.03\n",
      "loss= 3.534270615914926 2.074525685647592 1.459744930267334\n",
      "639 winrate= 0.78 0.04\n",
      "loss= 3.690526099820013 2.273190708775396 1.4173353910446167\n",
      "640 winrate= 0.75 0.03\n",
      "loss= 3.624287483170461 2.203442570641469 1.4208449125289917\n",
      "641 winrate= 0.79 0.06\n",
      "loss= 3.564648427003937 2.12051609897049 1.4441323280334473\n",
      "642 winrate= 0.78 0.03\n",
      "loss= 3.577353259947403 2.1891077965761263 1.3882454633712769\n",
      "643 winrate= 0.82 0.03\n",
      "loss= 3.63133882674065 2.155820361698536 1.4755184650421143\n",
      "ARENA!!!  0.45 0.24 0.31000000000000005\n",
      "upgrade 0.45 0.24 0.31000000000000005\n",
      "winrate against random  0.785 0.03\n",
      "644 winrate= 0.78 0.06\n",
      "loss= 3.4901731472404744 2.0811949234398153 1.4089782238006592\n",
      "645 winrate= 0.78 0.04\n",
      "loss= 3.4918002768500056 2.1082232876761164 1.3835769891738892\n",
      "646 winrate= 0.75 0.03\n",
      "loss= 3.68598124709437 2.2453576060802707 1.4406236410140991\n",
      "647 winrate= 0.78 0.01\n",
      "loss= 3.4894397224006446 2.048323389392832 1.4411163330078125\n",
      "648 winrate= 0.84 0.02\n",
      "loss= 3.598330010204004 2.1806208980366457 1.4177091121673584\n",
      "649 winrate= 0.76 0.02\n",
      "loss= 3.3583363995108084 1.9544136032614188 1.4039227962493896\n",
      "650 winrate= 0.73 0.02\n",
      "loss= 3.624937211153691 2.211854969141667 1.413082242012024\n",
      "ARENA!!!  0.39 0.28 0.32999999999999996\n",
      "upgrade 0.39 0.28 0.32999999999999996\n",
      "winrate against random  0.778 0.036\n",
      "651 winrate= 0.84 0.03\n",
      "loss= 3.5225851071457672 2.1327161324600983 1.389868974685669\n",
      "652 winrate= 0.73 0.06\n",
      "loss= 3.550200991590339 2.14135008331092 1.408850908279419\n",
      "653 winrate= 0.78 0.06\n",
      "loss= 3.7856374925744505 2.3307161277902098 1.4549213647842407\n",
      "654 winrate= 0.79 0.06\n",
      "loss= 3.8237626835878364 2.3740409895952217 1.4497216939926147\n",
      "655 winrate= 0.74 0.07\n",
      "loss= 3.727809768290947 2.3094872051195603 1.4183225631713867\n",
      "656 winrate= 0.75 0.01\n",
      "loss= 3.714646250647386 2.309479267042955 1.4051669836044312\n",
      "657 winrate= 0.77 0.05\n",
      "loss= 3.6276227416183633 2.1944593371536416 1.4331634044647217\n",
      "ARENA!!!  0.3 0.32 0.37999999999999995\n",
      "658 winrate= 0.7 0.04\n",
      "loss= 3.7356508419088357 2.298636381345176 1.4370144605636597\n",
      "659 winrate= 0.77 0.04\n",
      "loss= 3.5156233485966206 2.1362697061329365 1.379353642463684\n",
      "660 winrate= 0.76 0.04\n",
      "loss= 3.67219938144747 2.2428139577871917 1.4293854236602783\n",
      "661 winrate= 0.83 0.03\n",
      "loss= 3.7753917909403842 2.3687033391734165 1.4066884517669678\n",
      "662 winrate= 0.66 0.05\n",
      "loss= 3.733441548424592 2.3496542740639352 1.3837872743606567\n",
      "663 winrate= 0.78 0.06\n",
      "loss= 3.5923249803024415 2.1804413876968507 1.4118835926055908\n",
      "664 winrate= 0.83 0.03\n",
      "loss= 3.682866082826091 2.2727672917036545 1.4100987911224365\n",
      "ARENA!!!  0.34 0.32 0.3399999999999999\n",
      "upgrade 0.34 0.32 0.3399999999999999\n",
      "winrate against random  0.785 0.036\n",
      "665 winrate= 0.83 0.03\n",
      "loss= 3.6986844013741904 2.2666124771645957 1.4320719242095947\n",
      "666 winrate= 0.81 0.05\n",
      "loss= 3.6025826951426527 2.190489651815321 1.4120930433273315\n",
      "667 winrate= 0.78 0.01\n",
      "loss= 3.5738965535744143 2.1426618362053347 1.4312347173690796\n",
      "668 winrate= 0.82 0.03\n",
      "loss= 3.4995332789957754 2.0967136455118887 1.4028196334838867\n",
      "669 winrate= 0.75 0.01\n",
      "loss= 3.3687626440760314 1.952689798617047 1.4160728454589844\n",
      "670 winrate= 0.88 0.01\n",
      "loss= 3.5418844405787255 2.1025831882136132 1.4393012523651123\n",
      "671 winrate= 0.76 0.04\n",
      "loss= 3.6515302310701805 2.2051593909975487 1.4463708400726318\n",
      "ARENA!!!  0.4 0.25 0.35\n",
      "upgrade 0.4 0.25 0.35\n",
      "winrate against random  0.786 0.026\n",
      "672 winrate= 0.75 0.02\n",
      "loss= 3.4664864412863046 2.058171497972706 1.4083149433135986\n",
      "673 winrate= 0.77 0.02\n",
      "loss= 3.5949131874341345 2.165954533030448 1.4289586544036865\n",
      "674 winrate= 0.82 0.03\n",
      "loss= 3.603503705577796 2.1591888681220466 1.4443148374557495\n",
      "675 winrate= 0.77 0.03\n",
      "loss= 3.757413093183609 2.3515748420697173 1.4058382511138916\n",
      "676 winrate= 0.82 0.04\n",
      "loss= 3.6831368178081947 2.246730801458402 1.4364060163497925\n",
      "677 winrate= 0.8 0.01\n",
      "loss= 3.6417343338894383 2.2137098034786717 1.4280245304107666\n",
      "678 winrate= 0.75 0.04\n",
      "loss= 3.451246297633622 2.0492649438738564 1.4019813537597656\n",
      "ARENA!!!  0.44 0.25 0.31000000000000005\n",
      "upgrade 0.44 0.25 0.31000000000000005\n",
      "winrate against random  0.816 0.035\n",
      "679 winrate= 0.82 0.07\n",
      "loss= 3.6765861231654675 2.2214514929622204 1.455134630203247\n",
      "680 winrate= 0.8 0.05\n",
      "loss= 3.6055287650492165 2.1886499455835793 1.4168788194656372\n",
      "681 winrate= 0.75 0.02\n",
      "loss= 3.517464990628287 2.129360432637259 1.3881045579910278\n",
      "682 winrate= 0.81 0.03\n",
      "loss= 3.601562947564247 2.1866569226313857 1.4149060249328613\n",
      "683 winrate= 0.79 0.04\n",
      "loss= 3.441274049562187 2.0471575286805344 1.3941165208816528\n",
      "684 winrate= 0.69 0.06\n",
      "loss= 3.6535780733914174 2.2485615319103993 1.405016541481018\n",
      "685 winrate= 0.79 0.04\n",
      "loss= 3.6550166331328713 2.249416562179502 1.4056000709533691\n",
      "ARENA!!!  0.45 0.18 0.37000000000000005\n",
      "upgrade 0.45 0.18 0.37000000000000005\n",
      "winrate against random  0.774 0.039\n",
      "686 winrate= 0.81 0.01\n",
      "loss= 3.4374270029162037 2.0181539840792286 1.419273018836975\n",
      "687 winrate= 0.74 0.03\n",
      "loss= 3.5502526789053914 2.1102443962439534 1.440008282661438\n",
      "688 winrate= 0.78 0.05\n",
      "loss= 3.825444111052569 2.39903105181795 1.4264130592346191\n",
      "689 winrate= 0.81 0.01\n",
      "loss= 3.737299927537109 2.2950208271190182 1.4422791004180908\n",
      "690 winrate= 0.77 0.02\n",
      "loss= 3.589697064495369 2.154681982136055 1.435015082359314\n",
      "691 winrate= 0.78 0.02\n",
      "loss= 3.3773323980766135 1.9647602526145773 1.4125721454620361\n",
      "692 winrate= 0.78 0.04\n",
      "loss= 3.4469642978241026 2.0497937541534483 1.3971705436706543\n",
      "ARENA!!!  0.32 0.35 0.32999999999999996\n",
      "693 winrate= 0.8 0.0\n",
      "loss= 3.4052607549048957 2.011828018508106 1.3934327363967896\n",
      "694 winrate= 0.77 0.02\n",
      "loss= 3.4714475935467526 2.0857518976696774 1.3856956958770752\n",
      "695 winrate= 0.74 0.03\n",
      "loss= 3.657701642394981 2.259327800155601 1.3983738422393799\n",
      "696 winrate= 0.8 0.05\n",
      "loss= 3.7232364422228095 2.273798108711457 1.4494383335113525\n",
      "697 winrate= 0.66 0.05\n",
      "loss= 3.5149538542996033 2.116261127210675 1.3986927270889282\n",
      "698 winrate= 0.74 0.03\n",
      "loss= 3.7390658655025626 2.2714652814724112 1.4676005840301514\n",
      "699 winrate= 0.76 0.05\n",
      "loss= 3.6549930498474303 2.2199705765121642 1.4350224733352661\n",
      "ARENA!!!  0.37 0.22 0.41000000000000003\n",
      "700 winrate= 0.84 0.03\n",
      "loss= 3.6423864513546653 2.2239105850369163 1.418475866317749\n",
      "701 winrate= 0.79 0.01\n",
      "loss= 3.54917491241179 2.1316833571978617 1.4174915552139282\n",
      "702 winrate= 0.72 0.06\n",
      "loss= 3.495806126373582 2.09056499459105 1.4052411317825317\n",
      "703 winrate= 0.81 0.08\n",
      "loss= 3.627682054751992 2.1854246504245993 1.4422574043273926\n",
      "704 winrate= 0.76 0.08\n",
      "loss= 3.5864438178555793 2.1713439586179084 1.415099859237671\n",
      "705 winrate= 0.67 0.08\n",
      "loss= 3.6995757941903205 2.268636119812879 1.4309396743774414\n",
      "706 winrate= 0.8 0.05\n",
      "loss= 3.734157705710554 2.2868020776969433 1.4473556280136108\n",
      "ARENA!!!  0.32 0.28 0.3999999999999999\n",
      "707 winrate= 0.77 0.03\n",
      "loss= 3.5596987646282034 2.1455648582637625 1.414133906364441\n",
      "708 winrate= 0.76 0.03\n",
      "loss= 3.8714543090576408 2.4400884614700553 1.4313658475875854\n",
      "709 winrate= 0.77 0.03\n",
      "loss= 3.7549823667196134 2.346300854840646 1.4086815118789673\n",
      "710 winrate= 0.78 0.03\n",
      "loss= 3.4965178911061443 2.1031709853978313 1.393346905708313\n",
      "711 winrate= 0.82 0.01\n",
      "loss= 3.521024462917224 2.117923018672839 1.4031014442443848\n",
      "712 winrate= 0.76 0.03\n",
      "loss= 3.501993840593294 2.1241272148074666 1.3778666257858276\n",
      "713 winrate= 0.79 0.02\n",
      "loss= 3.385712454202011 1.9623326753400114 1.4233797788619995\n",
      "ARENA!!!  0.41 0.21 0.3800000000000001\n",
      "upgrade 0.41 0.21 0.3800000000000001\n",
      "winrate against random  0.797 0.037\n",
      "714 winrate= 0.79 0.02\n",
      "loss= 3.6287490231260437 2.2213449579939026 1.4074040651321411\n",
      "715 winrate= 0.83 0.02\n",
      "loss= 3.587896585712418 2.2061456444358676 1.3817509412765503\n",
      "716 winrate= 0.78 0.02\n",
      "loss= 3.5091981117894537 2.111431640880335 1.3977664709091187\n",
      "717 winrate= 0.82 0.08\n",
      "loss= 3.6468632799330147 2.2259179931822213 1.4209452867507935\n",
      "718 winrate= 0.72 0.02\n",
      "loss= 3.6444484212108 2.2184505679316864 1.4259978532791138\n",
      "719 winrate= 0.73 0.04\n",
      "loss= 3.389081747035942 1.966192514400444 1.422889232635498\n",
      "720 winrate= 0.8 0.02\n",
      "loss= 3.453731404878509 2.0629074445742486 1.3908239603042603\n",
      "ARENA!!!  0.41 0.28 0.31000000000000005\n",
      "upgrade 0.41 0.28 0.31000000000000005\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-12bd1335ec5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mbest_vfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"upgrade\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrawrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloserate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mwinrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrawrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_winrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"winrate against random \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrawrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e4e68071fc41>\u001b[0m in \u001b[0;36meval_winrate\u001b[0;34m(totest, bench, env, n_games)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrew2count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0migame\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_available_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mrsum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-fddece52dd3c>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, not_available_actions)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs2testorobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-fddece52dd3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, not_available_actions)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mnot_available_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "# rbuff = ReplayBuffer(nitems=3, max_len=50*9*8)\n",
    "bsize = 128\n",
    "# wll.new()\n",
    "writer = wll.writer\n",
    "opt = torch.optim.Adam(list(tree_policy.parameters())+list(value_function.parameters()), lr=1e-3)\n",
    "import copy\n",
    "best_tree_policy = copy.deepcopy(tree_policy)\n",
    "best_opt = copy.deepcopy(opt)\n",
    "best_vfunc = copy.deepcopy(value_function)\n",
    "loss = 0\n",
    "\n",
    "for game in range(10000):\n",
    "    game_step = 0\n",
    "    done = False\n",
    "    game_buff = [] \n",
    "    while not done:\n",
    "        game_step += 1\n",
    "        rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "        root = Node2(obs, 0, reward_sign=rew_sign)\n",
    "        dic = MCTS2(root, 10, 100, tree_policy, model, 100)\n",
    "        dic = np.array(dic)\n",
    "\n",
    "        tdic = torch.tensor([[-dic]])\n",
    "        monte_probs = torch.softmax(tdic*10, dim=-1).detach()\n",
    "        tensor_obs = tree_policy.obs2testorobs(obs).unsqueeze(0)\n",
    "        game_buff.append([tensor_obs, monte_probs, rew_sign])\n",
    "        \n",
    "        move = np.argmin(dic)\n",
    "        obs, r, done, _ = test_env.step(move)\n",
    "\n",
    "    symetric_add2rbuff(rbuff, game_buff, rew_sign)\n",
    "\n",
    "    if len(rbuff) > bsize:\n",
    "        for opt_step in range(4):\n",
    "            tensor_obs, monte_probs, game_finish = rbuff.get(bsize)\n",
    "            policy_probs = tree_policy(tensor_obs)\n",
    "            loss_policy = -(monte_probs*torch.log(policy_probs+1e-8)).mean()*2\n",
    "            loss_value  = ((value_function(tensor_obs)-game_finish)**2).mean()\n",
    "            loss = loss_policy + loss_value\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(\"loss=\", loss.item(), loss_policy.item(), loss_value.item())\n",
    "        writer.add_scalar('loss/loss', loss.item(), game)\n",
    "        writer.add_scalar('loss/policy', loss_policy.item(), game)\n",
    "        writer.add_scalar('loss/vfunc', loss_value.item(), game)\n",
    "    else:\n",
    "        print(len(rbuff))\n",
    "    if game % 7 == 0:\n",
    "        tree_policy.temperature, best_tree_policy.temperature = 1, 1\n",
    "        winrate, drawrate, loserate = eval_winrate(tree_policy, best_tree_policy, test_env, n_games=100)\n",
    "        tree_policy.temperature, best_tree_policy.temperature = 0.1, 0.1\n",
    "        print(\"ARENA!!! \", winrate, drawrate, loserate)\n",
    "        if winrate > loserate:\n",
    "            best_tree_policy = copy.deepcopy(tree_policy)\n",
    "            best_opt = copy.deepcopy(opt)\n",
    "            best_vfunc = copy.deepcopy(value_function)\n",
    "            print(\"upgrade\", winrate, drawrate, loserate)\n",
    "            winrate, drawrate, _ = eval_winrate(tree_policy, rollout_policy, test_env, n_games=1000)\n",
    "            print(\"winrate against random \", winrate, drawrate)\n",
    "        else:\n",
    "            tree_policy = copy.deepcopy(best_tree_policy)\n",
    "            opt = copy.deepcopy(best_opt)\n",
    "            value_function = copy.deepcopy(best_vfunc)\n",
    "\n",
    "        \n",
    "    winrate2, drawrate2, _ = eval_winrate(tree_policy, rollout_policy, test_env, n_games=100)\n",
    "    writer.add_scalar('winrate/winrate', winrate2, game)\n",
    "    writer.add_scalar('winrate/drawrate', drawrate2, game)\n",
    "    print(game, \"winrate=\", winrate2, drawrate2)\n",
    "\n",
    "    obs = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir run65\n",
    "# torch.save(tree_policy.state_dict(), 'run65/tree.ph')\n",
    "# torch.save(value_function.state_dict(), 'run65/value.ph')\n",
    "# torch.save(opt.state_dict(), 'run65/opt.ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0bb29030002e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtree_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run65/tree.ph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalue_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run65/value.ph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run65/opt.ph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "# tree_policy.load_state_dict(torch.load('run65/tree.ph'))\n",
    "# value_function.load_state_dict(torch.load('run65/value.ph'))\n",
    "# opt.load_state_dict(torch.load('run65/opt.ph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      "your action was 5\n",
      "   | | \n",
      "  -----\n",
      "   | |O\n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "   | | \n",
      "  -----\n",
      "   | |O\n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-46a14bd73f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbesta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_available_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhuman\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrew_sign\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbesta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your play? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your action was\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbesta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbesta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "\n",
    "random_policy = TreePolicy()\n",
    "human = 1\n",
    "\n",
    "while not done:\n",
    "    rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "    root = Node2(obs, 0, reward_sign=rew_sign)\n",
    "    besta = tree_policy.act(obs, model.not_available_actions(obs))\n",
    "    if human == rew_sign:\n",
    "        besta = int(input(\"your play? \"))\n",
    "        print(\"your action was\", besta)\n",
    "    obs, r, done, _ = test_env.step(besta)\n",
    "    test_env.render()\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  }
 ]
}