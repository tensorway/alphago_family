{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd95df06ad6614d8ea31ce2c635a29972",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_tictactoe.env import TicTacToeEnv, agent_by_mark, next_mark\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from myrl.buffers import ReplayBuffer\n",
    "from myrl.utils import ExperimentWriter\n",
    "from myrl.value_functions import \n",
    "\n",
    "env = TicTacToeEnv()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, obs, render=False):\n",
    "        return env.action_space.sample()\n",
    "    def rollout(self, obs, model, render=False):\n",
    "        d = False\n",
    "        rsum = 0\n",
    "        while not d:\n",
    "            obs, r, d, _ = model.step(obs, self.act(obs))\n",
    "            rsum += r\n",
    "        return rsum\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def step(self, obs, action):\n",
    "        self._set_env(obs)\n",
    "        return self.env.step(action)\n",
    "    def available_actions(self, obs):\n",
    "        self._set_env(obs)\n",
    "        return self.env.available_actions()\n",
    "    def _set_env(self, obs):\n",
    "        self.env.board = list(obs[0])\n",
    "        self.env.mark  = obs[1] \n",
    "        self.done = False\n",
    "    def get_num_actions(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "class TreePolicy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def act(self, obs, available_actions):\n",
    "        import random\n",
    "        return random.choice(available_actions)\n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        return [1/len(available_actions) for i in range(len(available_actions))]\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, net_arch, middle_activation=F.relu, last_activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = middle_activation(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        h = last_activation(h)\n",
    "        return h\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, net_arch, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = torch.tensor(x, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = middle_activation(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        return h\n",
    "\n",
    "class NNTreePolicy(nn.Module):\n",
    "    def __init__(self, net_arch, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, x):\n",
    "        h = torch.tensor(x, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            # if print:\n",
    "            #     print(h.shape, x.shape)\n",
    "            h = F.relu(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        h = torch.softmax(h, dim=1)\n",
    "        return h\n",
    "    def act(self, obs, available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs)\n",
    "        indices = h.sort(descending=True).indices[0]\n",
    "        # print(indices)\n",
    "        for i in indices:\n",
    "            if int(i.item()) in available_actions:\n",
    "                return int(i.item())        \n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs)\n",
    "        return h.tolist()[0]\n",
    "    def obs2testorobs(self, obs):\n",
    "        l2 = [1] if obs[1]=='O' else [-1]\n",
    "        obs = torch.tensor([list(obs[0])+l2])\n",
    "        obs[obs==2] = -1\n",
    "        return obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rollout_policy = RolloutPolicy(env)\n",
    "model = Model(TicTacToeEnv())\n",
    "backbone = Backbone([10, 16])\n",
    "value_function = ValueFunction([16, 4, 1], backbone=backbone)\n",
    "tree_policy = NNTreePolicy([16, 9, 9], backbone=backbone)\n",
    "\n",
    "# obs = env.reset()\n",
    "# rollout_policy.rollout(obs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "test_env.mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, obs, reward, change_child_rew_sign=True, reward_sign=1, done=False, parent=None):\n",
    "        self.n = 0\n",
    "        self.cumulative_reward = 0#reward\n",
    "        self.parent = parent\n",
    "        self.action2child = {}\n",
    "        self.nchildren = 0\n",
    "        self.taken_actions = []\n",
    "        self.obs = obs\n",
    "        self.done = done\n",
    "        self.reward = reward\n",
    "        self.reward_sign = reward_sign\n",
    "        self.change_child_rew_sign = change_child_rew_sign\n",
    "    def get_q(self):\n",
    "        return self.cumulative_reward/(self.n)\n",
    "\n",
    "    def backpropagate(self, r, gamma=1):\n",
    "        self.n += 1\n",
    "        self.cumulative_reward += r*gamma*self.reward_sign\n",
    "        if not( -1 <= self.get_q() <= 1 ) and 0:\n",
    "            print(self.__dict__)\n",
    "        if self.parent is None:\n",
    "            return \n",
    "        self.parent.backpropagate(r, gamma=gamma)\n",
    "    def print_parents(self):\n",
    "        if self.parent is None:\n",
    "            return\n",
    "        self.parent.print_parents()\n",
    "\n",
    "    def create_child(self, ChildType, policy, model):\n",
    "        non_taken_actions = self._list_cut(model.available_actions(self.obs), self.taken_actions)\n",
    "        action = policy.act(self.obs, non_taken_actions)\n",
    "        self.taken_actions.append(action)\n",
    "        obs, reward, done, info = model.step(self.obs, action)\n",
    "        reward_sign= -self.reward_sign if self.change_child_rew_sign else self.reward_sign\n",
    "        child = ChildType(obs, reward, done=done, reward_sign=reward_sign, parent=self, change_child_rew_sign=self.change_child_rew_sign)\n",
    "        self.action2child[action] = child\n",
    "        self.nchildren += 1\n",
    "        return child, action\n",
    "\n",
    "    def _list_cut(self, l1, l2):\n",
    "        toret = []\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret\n",
    "\n",
    "    def rollout(self, rollout_policy, model, render=False):\n",
    "        if self.done:\n",
    "            return self.reward\n",
    "        return rollout_policy.rollout(self.obs, model, render=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(root_node, policy, model, alpha=1):\n",
    "    scores = [0 for i in range(model.get_num_actions())]\n",
    "    all_actions = list(range(model.get_num_actions()))\n",
    "    probs = policy.get_action_probs(root_node.obs, all_actions)\n",
    "    maxscore, a_maxscore = float('-inf'), -1\n",
    "    for a in root_node.action2child.keys():\n",
    "        child = root_node.action2child[a]\n",
    "        u = probs[a]/(1+child.n)\n",
    "        q = child.get_q()\n",
    "        score = q + alpha*u\n",
    "        scores.append(score)\n",
    "        if score > maxscore:\n",
    "            maxscore = score\n",
    "            a_maxscore = a\n",
    "    return scores, (a_maxscore, maxscore)\n",
    "\n",
    "def MCTS(root_node, max_depth, n_times, policy, model, alpha_UCB=1):\n",
    "    current_node  = root_node\n",
    "    current_depth = 0\n",
    "    n_times_done  = 0\n",
    "\n",
    "    while n_times_done != n_times:\n",
    "        if current_depth == max_depth or current_node.done:\n",
    "            # current_node.print_parents()\n",
    "            reward = current_node.rollout(rollout_policy, model)\n",
    "            current_node.backpropagate(reward, gamma=0.99)\n",
    "            current_node = root_node\n",
    "            n_times_done += 1\n",
    "            current_depth = 0\n",
    "            model.env.done = False\n",
    "        elif current_node.nchildren < len(model.available_actions(current_node.obs)):\n",
    "            child, action = current_node.create_child(Node, policy, model) \n",
    "            current_node = child\n",
    "            current_depth += 1\n",
    "        else:\n",
    "            scores, (a_maxscore, maxscore) = UCB(current_node, policy, model, alpha_UCB)\n",
    "            current_node = current_node.action2child[a_maxscore]\n",
    "            current_depth += 1\n",
    "\n",
    "    visits = []\n",
    "    for a in range(model.get_num_actions()):\n",
    "        if a in root_node.action2child:\n",
    "            visits.append(root_node.action2child[a].get_q())\n",
    "        else:\n",
    "            visits.append(float('inf'))\n",
    "    return visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_winrate(totest, bench, env, n_games=100):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    for igame in range(n_games):\n",
    "        done, reward = False, 0\n",
    "        obs = env.reset()\n",
    "        curr_policy = totest if igame<=n_games//2 else bench\n",
    "        rew2count = 1 if igame<=n_games//2 else -1\n",
    "        while not done:\n",
    "            action = curr_policy.act(obs, env.available_actions())\n",
    "            obs, r, done, _ = test_env.step(action)\n",
    "            curr_policy = totest if curr_policy==bench else bench\n",
    "            wins += 1 if r==rew2count else 0\n",
    "            # print(r)\n",
    "        draws += 1 if r==0 else 0\n",
    "        # print(\" \")\n",
    "    winrate = wins/n_games\n",
    "    drawrate = draws/n_games\n",
    "    return winrate, drawrate, (1-winrate-drawrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.63, 0.04, 0.33)"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "eval_winrate(tree_policy, rollout_policy, test_env, n_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  X| | \n  -----\n   |O| \n  -----\n   | | \n\nmove probs=  tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [1., 0., 0.]], dtype=torch.float64)\n  X| | \n  -----\n   |O| \n  -----\n  O| | \n\nmove probs=  tensor([[0., 0., 0.],\n        [1., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\n  X| | \n  -----\n  X|O| \n  -----\n  O| | \n\nmove probs=  tensor([[0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\n  X| |O\n  -----\n  X|O| \n  -----\n  O| | \n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "obs, r, done, _ = test_env.step(4)\n",
    "obs, r, done, _ = test_env.step(0)\n",
    "# obs, r, done, _ = test_env.step(1)\n",
    "# obs, r, done, _ = test_env.step(7)\n",
    "# obs, r, done, _ = test_env.step(6)\n",
    "# obs, r, done, _ = test_env.step(2)\n",
    "# obs, r, done, _ = test_env.step(3)\n",
    "# obs, r, done, _ = test_env.step(5)\n",
    "done = False\n",
    "test_env.render()\n",
    "# rbuff = ReplayBuffer(200)\n",
    "\n",
    "while not done:\n",
    "    rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "    root = Node(obs, 0, reward_sign=rew_sign)\n",
    "    dic = MCTS(root, 10, 1, tree_policy, model, 100)\n",
    "    dic = np.array(dic)\n",
    "    tdic = torch.tensor([-dic])\n",
    "    print(\"move probs= \", torch.softmax(tdic, dim=1).view(3, 3))\n",
    "    move = np.argmin(dic)\n",
    "    # print(move, test_env.mark)\n",
    "    # print(dic.reshape(3, 3))\n",
    "    obs, r, done, _ = test_env.step(move)\n",
    "    test_env.render()\n",
    "    # print(\" \")\n",
    "    # print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      "upgrade 0.501 0.0 0.499\n",
      "0 winrate= 0.58 0.11\n",
      "upgrade 0.501 0.0 0.499\n",
      "1 winrate= 0.6 0.06\n",
      "upgrade 0.501 0.0 0.499\n",
      "2 winrate= 0.59 0.06\n",
      "upgrade 0.501 0.0 0.499\n",
      "3 winrate= 0.56 0.05\n",
      "upgrade 0.501 0.0 0.499\n",
      "4 winrate= 0.59 0.06\n",
      "upgrade 0.501 0.0 0.499\n",
      "5 winrate= 0.54 0.09\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-7358fa23c1df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mrew_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_mark\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrew_sign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-6b3094ac8722>\u001b[0m in \u001b[0;36mMCTS\u001b[0;34m(root_node, max_depth, n_times, policy, model, alpha_UCB)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_maxscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxscore\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUCB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_UCB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction2child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma_maxscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-6b3094ac8722>\u001b[0m in \u001b[0;36mUCB\u001b[0;34m(root_node, policy, model, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mall_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmaxscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_maxscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction2child\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-c4c624816496>\u001b[0m in \u001b[0;36mget_action_probs\u001b[0;34m(self, obs, available_actions)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs2testorobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobs2testorobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-c4c624816496>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# if print:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "# rbuff = ReplayBuffer(nitems=2, max_len=400)\n",
    "bsize = 32\n",
    "wll = ExperimentWriter('tb/alpha_tictacte_zero_3_')\n",
    "wll.new()\n",
    "writer = wll.writer\n",
    "# opt = torch.optim.Adam(tree_policy.parameters(), lr=1e-3)\n",
    "import copy\n",
    "# best_tree_policy = copy.deepcopy(tree_policy)\n",
    "# best_opt = copy.deepcopy(opt)\n",
    "\n",
    "for game in range(10000):\n",
    "    game_step = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        game_step += 1\n",
    "        rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "        root = Node(obs, 0, reward_sign=rew_sign)\n",
    "        dic = MCTS(root, 10, 100, tree_policy, model, 100)\n",
    "        dic = np.array(dic)\n",
    "\n",
    "        tdic = torch.tensor([[-dic]])\n",
    "        monte_probs = torch.softmax(tdic, dim=-1).detach()\n",
    "        tensor_obs = tree_policy.obs2testorobs(obs).unsqueeze(0)\n",
    "        rbuff.add(tensor_obs, monte_probs)\n",
    "        \n",
    "        move = np.argmin(dic)\n",
    "        obs, r, done, _ = test_env.step(move)\n",
    "\n",
    "        if len(rbuff) > bsize:\n",
    "            for opt_step in range(2):\n",
    "                tensor_obs, monte_probs = rbuff.get(bsize)\n",
    "                policy_probs = tree_policy(tensor_obs)\n",
    "                loss = -(monte_probs*torch.log(policy_probs+1e-8)).mean()\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            writer.add_scalar('loss', loss.item(), game)\n",
    "    winrate, drawrate, loserate = eval_winrate(tree_policy, best_tree_policy, test_env, n_games=1000)\n",
    "    if winrate > loserate:\n",
    "        best_tree_policy = copy.deepcopy(tree_policy)\n",
    "        best_opt = copy.deepcopy(opt)\n",
    "        print(\"upgrade\", winrate, drawrate, loserate)\n",
    "    else:\n",
    "        tree_policy = copy.deepcopy(best_tree_policy)\n",
    "        opt = copy.deepcopy(best_opt)\n",
    "        \n",
    "    winrate, drawrate, _ = eval_winrate(tree_policy, rollout_policy, test_env, n_games=100)\n",
    "    writer.add_scalar('winrate/winrate', winrate, game)\n",
    "    writer.add_scalar('winrate/drawrate', drawrate, game)\n",
    "    print(game, \"winrate=\", winrate, drawrate)\n",
    "\n",
    "    obs = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbuff.deqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(rbuff.add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NNTreePolicy(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (1): Linear(in_features=9, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "tree_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}