{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd95df06ad6614d8ea31ce2c635a29972",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_tictactoe.env import TicTacToeEnv, agent_by_mark, next_mark\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from myrl.buffers import ReplayBuffer\n",
    "from myrl.utils import ExperimentWriter\n",
    "# from myrl.value_functions import \n",
    "\n",
    "env = TicTacToeEnv()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, obs, render=False):\n",
    "        return env.action_space.sample()\n",
    "    def rollout(self, obs, model, render=False):\n",
    "        d = False\n",
    "        rsum = 0\n",
    "        while not d:\n",
    "            obs, r, d, _ = model.step(obs, self.act(obs))\n",
    "            rsum += r\n",
    "        return rsum\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def step(self, obs, action):\n",
    "        self._set_env(obs)\n",
    "        return self.env.step(action)\n",
    "    def available_actions(self, obs):\n",
    "        self._set_env(obs)\n",
    "        return self.env.available_actions()\n",
    "    def not_available_actions(self, obs):\n",
    "        return self._list_cut(list(range(self.get_num_actions())), self.available_actions(obs))\n",
    "    def _list_cut(self, l1, l2):\n",
    "        toret = []\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret\n",
    "    def _set_env(self, obs):\n",
    "        self.env.board = list(obs[0])\n",
    "        self.env.mark  = obs[1] \n",
    "        self.done = False\n",
    "    def get_num_actions(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "class TreePolicy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def act(self, obs, available_actions):\n",
    "        import random\n",
    "        return random.choice(available_actions)\n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        return [1/len(available_actions) for i in range(len(available_actions))]\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, net_arch, middle_activation=F.relu, last_activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.middle_activation = middle_activation\n",
    "        self.last_activation = last_activation\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = self.middle_activation(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        h = self.last_activation(h)\n",
    "        return h\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, net_arch, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = torch.tensor(h, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = F.relu(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        return h\n",
    "\n",
    "class NNTreePolicy(nn.Module):\n",
    "    def __init__(self, net_arch, backbone, temperature=1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.temperature = temperature\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, x, not_available_actions=None):\n",
    "        h = torch.tensor(x, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = F.relu(lay(h))\n",
    "        h = self.layers[-1](h)/self.temperature\n",
    "        if not_available_actions is not None and len(not_available_actions)>0:\n",
    "            not_available_actions = torch.tensor(not_available_actions)\n",
    "            try:\n",
    "                h[0, not_available_actions] = float('-inf')\n",
    "            except:\n",
    "                print(not_available_actions)\n",
    "                print(h)\n",
    "                raise\n",
    "        h = torch.softmax(h, dim=1)\n",
    "        return h\n",
    "    def act(self, obs, not_available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs, not_available_actions=not_available_actions)\n",
    "        action = np.random.choice(range(len(h[0])), p=h.detach().squeeze(0).numpy())\n",
    "        return action      \n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs)\n",
    "        return h.tolist()[0]\n",
    "    def obs2testorobs(self, obs):\n",
    "        l2 = [1] if obs[1]=='O' else [-1]\n",
    "        obs = torch.tensor([list(obs[0])+l2])\n",
    "        obs[obs==2] = -1\n",
    "        return obs\n",
    "\n",
    "rollout_policy = RolloutPolicy(env)\n",
    "model = Model(TicTacToeEnv())\n",
    "backbone = Backbone([10, 16])\n",
    "value_function = ValueFunction([16, 4, 1], backbone=backbone)\n",
    "tree_policy = NNTreePolicy([16, 9, 9], backbone=backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, obs, reward, change_child_rew_sign=True, reward_sign=1, done=False, parent=None):\n",
    "        self.n = 0\n",
    "        self.cumulative_reward = 0#reward\n",
    "        self.parent = parent\n",
    "        self.action2child = {}\n",
    "        self.nchildren = 0\n",
    "        self.taken_actions = []\n",
    "        self.obs = obs\n",
    "        self.done = done\n",
    "        self.reward = reward\n",
    "        self.reward_sign = reward_sign\n",
    "        self.change_child_rew_sign = change_child_rew_sign\n",
    "        self.nzeros = 0\n",
    "        self.nones = 0\n",
    "        self.nmones = 0\n",
    "    def get_q(self):\n",
    "        return self.cumulative_reward/(self.n)\n",
    "\n",
    "    def backpropagate(self, r, gamma=1):\n",
    "        self.n += 1\n",
    "        self.cumulative_reward += r*gamma*self.reward_sign\n",
    "        self.nzeros += 1 if r==0 else 0\n",
    "        self.nones += 1 if r*self.reward_sign==1  else 0\n",
    "        self.nmones+= 1 if r*self.reward_sign==-1 else 0\n",
    "        if not( -1 <= self.get_q() <= 1 ) and 0:\n",
    "            print(self.__dict__)\n",
    "        if self.parent is None:\n",
    "            return \n",
    "        self.parent.backpropagate(r, gamma=gamma)\n",
    "    def print_parents(self):\n",
    "        if self.parent is None:\n",
    "            return\n",
    "        self.parent.print_parents()\n",
    "\n",
    "    def create_child(self, ChildType, policy, model):\n",
    "        not_available_actions = self._list_union(self.taken_actions, model.not_available_actions(self.obs))\n",
    "        # print(not_available_actions, self.parent)\n",
    "        action = policy.act(self.obs, not_available_actions)\n",
    "        self.taken_actions.append(action)\n",
    "        obs, reward, done, info = model.step(self.obs, action)\n",
    "        reward_sign= -self.reward_sign if self.change_child_rew_sign else self.reward_sign\n",
    "        child = ChildType(obs, reward, done=done, reward_sign=reward_sign, parent=self, change_child_rew_sign=self.change_child_rew_sign)\n",
    "        self.action2child[action] = child\n",
    "        self.nchildren += 1\n",
    "        return child, action\n",
    "\n",
    "    def _list_cut(self, l1, l2):\n",
    "        toret = []\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret\n",
    "    def _list_union(self, l1, l2):\n",
    "        toret = copy.deepcopy(l2)\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret       \n",
    "\n",
    "    def rollout(self, rollout_policy, model, render=False):\n",
    "        if self.done:\n",
    "            return self.reward\n",
    "        return rollout_policy.rollout(self.obs, model, render=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(root_node, policy, model, alpha=1):\n",
    "    scores = [0 for i in range(model.get_num_actions())]\n",
    "    all_actions = list(range(model.get_num_actions()))\n",
    "    probs = policy.get_action_probs(root_node.obs, all_actions)\n",
    "    minscore, a_minscore = float('inf'), -1\n",
    "    for a in root_node.action2child.keys():\n",
    "        child = root_node.action2child[a]\n",
    "        u = -probs[a]/(1+child.n)\n",
    "        q = child.get_q()\n",
    "        score = q + alpha*u\n",
    "        scores.append(score)\n",
    "        if score < minscore:\n",
    "            minscore = score\n",
    "            a_minscore = a\n",
    "    return scores, (a_minscore, minscore)\n",
    "\n",
    "def MCTS(root_node, max_depth, n_times, policy, model, alpha_UCB=1):\n",
    "    current_node  = root_node\n",
    "    current_depth = 0\n",
    "    n_times_done  = 0\n",
    "\n",
    "    while n_times_done != n_times:\n",
    "        if current_depth == max_depth or current_node.done:\n",
    "            reward = current_node.rollout(rollout_policy, model)\n",
    "            current_node.backpropagate(reward, gamma=0.99)\n",
    "            current_node = root_node\n",
    "            n_times_done += 1\n",
    "            current_depth = 0\n",
    "            model.env.done = False\n",
    "        elif current_node.nchildren < len(model.available_actions(current_node.obs)):\n",
    "            child, action = current_node.create_child(Node, policy, model) \n",
    "            current_node = child\n",
    "            current_depth += 1\n",
    "        else:\n",
    "            scores, (a_minscore, minscore) = UCB(current_node, policy, model, alpha_UCB)\n",
    "            current_node = current_node.action2child[a_minscore]\n",
    "            current_depth += 1\n",
    "\n",
    "    visits = []\n",
    "    not_available_actions = root_node._list_union(root_node.taken_actions, model.not_available_actions(root_node.obs))\n",
    "    for a in range(model.get_num_actions()):\n",
    "        if a in root_node.action2child:\n",
    "            visits.append(root_node.action2child[a].get_q())\n",
    "        else:\n",
    "            visits.append(float('inf'))\n",
    "    return visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_winrate(totest, bench, env, n_games=100):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    rsum = 0\n",
    "    for igame in range(n_games):\n",
    "        done, reward = False, 0\n",
    "        obs = env.reset()\n",
    "        curr_policy = totest if igame<=n_games//2 else bench\n",
    "        rew2count = 1 if igame<=n_games//2 else -1\n",
    "        while not done:\n",
    "            action = curr_policy.act(obs, model.not_available_actions(obs))\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            rsum += r\n",
    "            curr_policy = totest if curr_policy==bench else bench\n",
    "            wins += 1 if r==rew2count else 0\n",
    "        draws += 1 if r==0 else 0\n",
    "    winrate = wins/n_games\n",
    "    drawrate = draws/n_games\n",
    "    return winrate, drawrate, (1-winrate-drawrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      "move probs=  tensor([[0.0865, 0.0321, 0.2328],\n",
      "        [0.2328, 0.0865, 0.0321],\n",
      "        [0.2328, 0.0321, 0.0321]], dtype=torch.float64)\n",
      "2 O\n",
      "[[ 0.    0.99 -0.99]\n",
      " [-0.99  0.    0.99]\n",
      " [-0.99  0.99  0.99]]\n",
      "   | |O\n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0567, 0.1527, 0.0000],\n",
      "        [0.0567, 0.1527, 0.0567],\n",
      "        [0.4109, 0.0567, 0.0567]], dtype=torch.float64)\n",
      "6 X\n",
      "[[ 0.99  0.     inf]\n",
      " [ 0.99  0.    0.99]\n",
      " [-0.99  0.99  0.99]]\n",
      "   | |O\n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0874, 0.2351, 0.0000],\n",
      "        [0.0874, 0.0874, 0.2351],\n",
      "        [0.0000, 0.0325, 0.2351]], dtype=torch.float64)\n",
      "1 O\n",
      "[[ 0.   -0.99   inf]\n",
      " [ 0.    0.   -0.99]\n",
      " [  inf  0.99 -0.99]]\n",
      "   |O|O\n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.5496, 0.0000, 0.0000],\n",
      "        [0.0759, 0.1468, 0.0759],\n",
      "        [0.0000, 0.0759, 0.0759]], dtype=torch.float64)\n",
      "0 X\n",
      "[[-0.99   inf   inf]\n",
      " [ 0.99  0.33  0.99]\n",
      " [  inf  0.99  0.99]]\n",
      "  X|O|O\n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.2672, 0.2672, 0.0993],\n",
      "        [0.0000, 0.2672, 0.0993]], dtype=torch.float64)\n",
      "3 O\n",
      "[[ inf  inf  inf]\n",
      " [0.   0.   0.99]\n",
      " [ inf 0.   0.99]]\n",
      "  X|O|O\n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2456, 0.1917],\n",
      "        [0.0000, 0.3710, 0.1917]], dtype=torch.float64)\n",
      "7 X\n",
      "[[    inf     inf     inf]\n",
      " [    inf -0.2475  0.    ]\n",
      " [    inf -0.66    0.    ]]\n",
      "  X|O|O\n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X|X| \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1643, 0.2695],\n",
      "        [0.0000, 0.0000, 0.5662]], dtype=torch.float64)\n",
      "8 O\n",
      "[[    inf     inf     inf]\n",
      " [    inf  0.495   0.    ]\n",
      " [    inf     inf -0.7425]]\n",
      "  X|O|O\n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X|X|O\n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2709, 0.7291],\n",
      "        [0.0000, 0.0000, 0.0000]], dtype=torch.float64)\n",
      "5 X\n",
      "[[ inf  inf  inf]\n",
      " [ inf 0.99 0.  ]\n",
      " [ inf  inf  inf]]\n",
      "  X|O|O\n",
      "  -----\n",
      "  O| |X\n",
      "  -----\n",
      "  X|X|O\n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "4 O\n",
      "[[inf inf inf]\n",
      " [inf  0. inf]\n",
      " [inf inf inf]]\n",
      "  X|O|O\n",
      "  -----\n",
      "  O|O|X\n",
      "  -----\n",
      "  X|X|O\n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "# rbuff = ReplayBuffer(200)\n",
    "random_policy = TreePolicy()\n",
    "\n",
    "while not done:\n",
    "    rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "    root = Node(obs, 0, reward_sign=rew_sign)\n",
    "    dic = MCTS(root, 10, 10, tree_policy, model, 100)\n",
    "    dic = np.array(dic)\n",
    "    tdic = torch.tensor([-dic])\n",
    "    print(\"move probs= \", torch.softmax(tdic, dim=1).view(3, 3))\n",
    "    move = np.argmin(dic)\n",
    "    print(move, test_env.mark)\n",
    "    print(dic.reshape(3, 3))\n",
    "    obs, r, done, _ = test_env.step(move)\n",
    "    test_env.render()\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_policy = RolloutPolicy(env)\n",
    "model = Model(TicTacToeEnv())\n",
    "backbone = Backbone([10, 16])\n",
    "value_function = ValueFunction([16, 4, 1], backbone=backbone)\n",
    "tree_policy = NNTreePolicy([16, 9, 9], backbone=backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = copy.deepcopy(tree_policy)\n",
    "# wll = ExperimentWriter('tb/alpha_tictacte_zero_valuef_accurateMCTS_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      "9\n",
      "ARENA!!!  0.46 0.12 0.42000000000000004\n",
      "upgrade 0.46 0.12 0.42000000000000004\n",
      "0 winrate= 0.46 0.12 0.52 0.11\n",
      "16\n",
      "1 winrate= 0.46 0.12 0.52 0.08\n",
      "25\n",
      "2 winrate= 0.46 0.12 0.51 0.08\n",
      "34\n",
      "3 winrate= 0.46 0.12 0.53 0.11\n",
      "43\n",
      "4 winrate= 0.46 0.12 0.58 0.08\n",
      "52\n",
      "ARENA!!!  0.42 0.22 0.3600000000000001\n",
      "upgrade 0.42 0.22 0.3600000000000001\n",
      "5 winrate= 0.42 0.22 0.51 0.08\n",
      "61\n",
      "6 winrate= 0.42 0.22 0.48 0.06\n",
      "66\n",
      "7 winrate= 0.42 0.22 0.51 0.06\n",
      "71\n",
      "8 winrate= 0.42 0.22 0.53 0.08\n",
      "76\n",
      "9 winrate= 0.42 0.22 0.6 0.09\n",
      "83\n",
      "ARENA!!!  0.48 0.11 0.41000000000000003\n",
      "upgrade 0.48 0.11 0.41000000000000003\n",
      "10 winrate= 0.48 0.11 0.67 0.09\n",
      "92\n",
      "11 winrate= 0.48 0.11 0.53 0.11\n",
      "99\n",
      "12 winrate= 0.48 0.11 0.54 0.07\n",
      "108\n",
      "13 winrate= 0.48 0.11 0.5 0.1\n",
      "117\n",
      "14 winrate= 0.48 0.11 0.48 0.09\n",
      "122\n",
      "ARENA!!!  0.5 0.07 0.43\n",
      "upgrade 0.5 0.07 0.43\n",
      "15 winrate= 0.5 0.07 0.56 0.09\n",
      "16 winrate= 0.5 0.07 0.57 0.05\n",
      "17 winrate= 0.5 0.07 0.49 0.08\n",
      "18 winrate= 0.5 0.07 0.55 0.03\n",
      "19 winrate= 0.5 0.07 0.53 0.06\n",
      "ARENA!!!  0.42 0.1 0.4800000000000001\n",
      "20 winrate= 0.42 0.1 0.58 0.05\n",
      "21 winrate= 0.42 0.1 0.46 0.1\n",
      "22 winrate= 0.42 0.1 0.48 0.09\n",
      "23 winrate= 0.42 0.1 0.53 0.09\n",
      "24 winrate= 0.42 0.1 0.63 0.08\n",
      "ARENA!!!  0.42 0.16 0.42000000000000004\n",
      "25 winrate= 0.42 0.16 0.61 0.03\n",
      "26 winrate= 0.42 0.16 0.54 0.07\n",
      "27 winrate= 0.42 0.16 0.57 0.06\n",
      "28 winrate= 0.42 0.16 0.64 0.1\n",
      "29 winrate= 0.42 0.16 0.47 0.1\n",
      "ARENA!!!  0.44 0.11 0.45000000000000007\n",
      "30 winrate= 0.44 0.11 0.61 0.06\n",
      "31 winrate= 0.44 0.11 0.57 0.06\n",
      "32 winrate= 0.44 0.11 0.62 0.07\n",
      "33 winrate= 0.44 0.11 0.49 0.07\n",
      "34 winrate= 0.44 0.11 0.53 0.06\n",
      "ARENA!!!  0.43 0.1 0.4700000000000001\n",
      "35 winrate= 0.43 0.1 0.47 0.11\n",
      "36 winrate= 0.43 0.1 0.56 0.08\n",
      "37 winrate= 0.43 0.1 0.54 0.05\n",
      "38 winrate= 0.43 0.1 0.52 0.06\n",
      "39 winrate= 0.43 0.1 0.56 0.09\n",
      "ARENA!!!  0.59 0.11 0.30000000000000004\n",
      "upgrade 0.59 0.11 0.30000000000000004\n",
      "40 winrate= 0.59 0.11 0.53 0.09\n",
      "41 winrate= 0.59 0.11 0.52 0.07\n",
      "42 winrate= 0.59 0.11 0.5 0.07\n",
      "43 winrate= 0.59 0.11 0.57 0.12\n",
      "44 winrate= 0.59 0.11 0.61 0.07\n",
      "ARENA!!!  0.45 0.08 0.47000000000000003\n",
      "45 winrate= 0.45 0.08 0.6 0.03\n",
      "46 winrate= 0.45 0.08 0.59 0.06\n",
      "47 winrate= 0.45 0.08 0.6 0.06\n",
      "48 winrate= 0.45 0.08 0.6 0.05\n",
      "49 winrate= 0.45 0.08 0.56 0.07\n",
      "ARENA!!!  0.45 0.12 0.43000000000000005\n",
      "upgrade 0.45 0.12 0.43000000000000005\n",
      "50 winrate= 0.45 0.12 0.54 0.11\n",
      "51 winrate= 0.45 0.12 0.48 0.08\n",
      "52 winrate= 0.45 0.12 0.55 0.1\n",
      "53 winrate= 0.45 0.12 0.54 0.06\n",
      "54 winrate= 0.45 0.12 0.57 0.07\n",
      "ARENA!!!  0.47 0.14 0.39\n",
      "upgrade 0.47 0.14 0.39\n",
      "55 winrate= 0.47 0.14 0.62 0.02\n",
      "56 winrate= 0.47 0.14 0.57 0.04\n",
      "57 winrate= 0.47 0.14 0.58 0.05\n",
      "58 winrate= 0.47 0.14 0.56 0.08\n",
      "59 winrate= 0.47 0.14 0.51 0.07\n",
      "ARENA!!!  0.54 0.1 0.36\n",
      "upgrade 0.54 0.1 0.36\n",
      "60 winrate= 0.54 0.1 0.56 0.1\n",
      "61 winrate= 0.54 0.1 0.5 0.08\n",
      "62 winrate= 0.54 0.1 0.5 0.07\n",
      "63 winrate= 0.54 0.1 0.56 0.06\n",
      "64 winrate= 0.54 0.1 0.46 0.11\n",
      "ARENA!!!  0.48 0.14 0.38\n",
      "upgrade 0.48 0.14 0.38\n",
      "65 winrate= 0.48 0.14 0.56 0.09\n",
      "66 winrate= 0.48 0.14 0.45 0.09\n",
      "67 winrate= 0.48 0.14 0.53 0.06\n",
      "68 winrate= 0.48 0.14 0.49 0.06\n",
      "69 winrate= 0.48 0.14 0.48 0.12\n",
      "ARENA!!!  0.46 0.13 0.41000000000000003\n",
      "upgrade 0.46 0.13 0.41000000000000003\n",
      "70 winrate= 0.46 0.13 0.47 0.08\n",
      "71 winrate= 0.46 0.13 0.55 0.12\n",
      "72 winrate= 0.46 0.13 0.53 0.09\n",
      "73 winrate= 0.46 0.13 0.51 0.09\n",
      "74 winrate= 0.46 0.13 0.59 0.05\n",
      "ARENA!!!  0.46 0.14 0.4\n",
      "upgrade 0.46 0.14 0.4\n",
      "75 winrate= 0.46 0.14 0.63 0.08\n",
      "76 winrate= 0.46 0.14 0.54 0.11\n",
      "77 winrate= 0.46 0.14 0.5 0.08\n",
      "78 winrate= 0.46 0.14 0.6 0.08\n",
      "79 winrate= 0.46 0.14 0.54 0.03\n",
      "ARENA!!!  0.37 0.18 0.45\n",
      "80 winrate= 0.37 0.18 0.57 0.05\n",
      "81 winrate= 0.37 0.18 0.49 0.11\n",
      "82 winrate= 0.37 0.18 0.54 0.07\n",
      "83 winrate= 0.37 0.18 0.52 0.07\n",
      "84 winrate= 0.37 0.18 0.52 0.06\n",
      "ARENA!!!  0.43 0.16 0.41000000000000003\n",
      "upgrade 0.43 0.16 0.41000000000000003\n",
      "85 winrate= 0.43 0.16 0.56 0.08\n",
      "86 winrate= 0.43 0.16 0.62 0.04\n",
      "87 winrate= 0.43 0.16 0.58 0.06\n",
      "88 winrate= 0.43 0.16 0.58 0.08\n",
      "89 winrate= 0.43 0.16 0.62 0.09\n",
      "ARENA!!!  0.42 0.13 0.45000000000000007\n",
      "90 winrate= 0.42 0.13 0.5 0.08\n",
      "91 winrate= 0.42 0.13 0.53 0.05\n",
      "92 winrate= 0.42 0.13 0.52 0.11\n",
      "93 winrate= 0.42 0.13 0.47 0.09\n",
      "94 winrate= 0.42 0.13 0.49 0.1\n",
      "ARENA!!!  0.45 0.12 0.43000000000000005\n",
      "upgrade 0.45 0.12 0.43000000000000005\n",
      "95 winrate= 0.45 0.12 0.54 0.09\n",
      "96 winrate= 0.45 0.12 0.51 0.12\n",
      "97 winrate= 0.45 0.12 0.55 0.01\n",
      "98 winrate= 0.45 0.12 0.48 0.07\n",
      "99 winrate= 0.45 0.12 0.56 0.1\n",
      "ARENA!!!  0.46 0.1 0.44000000000000006\n",
      "upgrade 0.46 0.1 0.44000000000000006\n",
      "100 winrate= 0.46 0.1 0.57 0.09\n",
      "101 winrate= 0.46 0.1 0.55 0.06\n",
      "102 winrate= 0.46 0.1 0.47 0.05\n",
      "103 winrate= 0.46 0.1 0.56 0.08\n",
      "104 winrate= 0.46 0.1 0.61 0.05\n",
      "ARENA!!!  0.47 0.14 0.39\n",
      "upgrade 0.47 0.14 0.39\n",
      "105 winrate= 0.47 0.14 0.53 0.08\n",
      "106 winrate= 0.47 0.14 0.52 0.08\n",
      "107 winrate= 0.47 0.14 0.51 0.09\n",
      "108 winrate= 0.47 0.14 0.54 0.1\n",
      "109 winrate= 0.47 0.14 0.56 0.1\n",
      "ARENA!!!  0.43 0.19 0.38000000000000006\n",
      "upgrade 0.43 0.19 0.38000000000000006\n",
      "110 winrate= 0.43 0.19 0.6 0.08\n",
      "111 winrate= 0.43 0.19 0.62 0.04\n",
      "112 winrate= 0.43 0.19 0.57 0.05\n",
      "113 winrate= 0.43 0.19 0.5 0.07\n",
      "114 winrate= 0.43 0.19 0.51 0.11\n",
      "ARENA!!!  0.35 0.1 0.55\n",
      "115 winrate= 0.35 0.1 0.62 0.12\n",
      "116 winrate= 0.35 0.1 0.54 0.09\n",
      "117 winrate= 0.35 0.1 0.56 0.12\n",
      "118 winrate= 0.35 0.1 0.53 0.07\n",
      "119 winrate= 0.35 0.1 0.42 0.08\n",
      "ARENA!!!  0.42 0.15 0.43000000000000005\n",
      "120 winrate= 0.42 0.15 0.51 0.09\n",
      "121 winrate= 0.42 0.15 0.55 0.07\n",
      "122 winrate= 0.42 0.15 0.6 0.06\n",
      "123 winrate= 0.42 0.15 0.54 0.09\n",
      "124 winrate= 0.42 0.15 0.57 0.1\n",
      "ARENA!!!  0.48 0.12 0.4\n",
      "upgrade 0.48 0.12 0.4\n",
      "125 winrate= 0.48 0.12 0.51 0.09\n",
      "126 winrate= 0.48 0.12 0.46 0.08\n",
      "127 winrate= 0.48 0.12 0.56 0.1\n",
      "128 winrate= 0.48 0.12 0.59 0.08\n",
      "129 winrate= 0.48 0.12 0.67 0.04\n",
      "ARENA!!!  0.43 0.14 0.43000000000000005\n",
      "130 winrate= 0.43 0.14 0.57 0.06\n",
      "131 winrate= 0.43 0.14 0.54 0.07\n",
      "132 winrate= 0.43 0.14 0.58 0.08\n",
      "133 winrate= 0.43 0.14 0.54 0.05\n",
      "134 winrate= 0.43 0.14 0.61 0.06\n",
      "ARENA!!!  0.37 0.18 0.45\n",
      "135 winrate= 0.37 0.18 0.56 0.07\n",
      "136 winrate= 0.37 0.18 0.58 0.06\n",
      "137 winrate= 0.37 0.18 0.59 0.11\n",
      "138 winrate= 0.37 0.18 0.57 0.09\n",
      "139 winrate= 0.37 0.18 0.42 0.1\n",
      "ARENA!!!  0.4 0.11 0.49\n",
      "140 winrate= 0.4 0.11 0.52 0.09\n",
      "141 winrate= 0.4 0.11 0.59 0.1\n",
      "142 winrate= 0.4 0.11 0.54 0.07\n",
      "143 winrate= 0.4 0.11 0.54 0.11\n",
      "144 winrate= 0.4 0.11 0.53 0.06\n",
      "ARENA!!!  0.41 0.09 0.5000000000000001\n",
      "145 winrate= 0.41 0.09 0.55 0.05\n",
      "146 winrate= 0.41 0.09 0.51 0.09\n",
      "147 winrate= 0.41 0.09 0.5 0.07\n",
      "148 winrate= 0.41 0.09 0.59 0.06\n",
      "149 winrate= 0.41 0.09 0.51 0.07\n",
      "ARENA!!!  0.41 0.18 0.4100000000000001\n",
      "150 winrate= 0.41 0.18 0.62 0.02\n",
      "151 winrate= 0.41 0.18 0.55 0.06\n",
      "152 winrate= 0.41 0.18 0.58 0.08\n",
      "153 winrate= 0.41 0.18 0.51 0.1\n",
      "154 winrate= 0.41 0.18 0.58 0.04\n",
      "ARENA!!!  0.5 0.09 0.41000000000000003\n",
      "upgrade 0.5 0.09 0.41000000000000003\n",
      "155 winrate= 0.5 0.09 0.58 0.06\n",
      "156 winrate= 0.5 0.09 0.61 0.08\n",
      "157 winrate= 0.5 0.09 0.58 0.06\n",
      "158 winrate= 0.5 0.09 0.61 0.07\n",
      "159 winrate= 0.5 0.09 0.51 0.08\n",
      "ARENA!!!  0.39 0.12 0.49\n",
      "160 winrate= 0.39 0.12 0.49 0.07\n",
      "161 winrate= 0.39 0.12 0.55 0.07\n",
      "162 winrate= 0.39 0.12 0.53 0.08\n",
      "163 winrate= 0.39 0.12 0.48 0.09\n",
      "164 winrate= 0.39 0.12 0.6 0.06\n",
      "ARENA!!!  0.42 0.15 0.43000000000000005\n",
      "165 winrate= 0.42 0.15 0.5 0.1\n",
      "166 winrate= 0.42 0.15 0.55 0.05\n",
      "167 winrate= 0.42 0.15 0.55 0.11\n",
      "168 winrate= 0.42 0.15 0.52 0.11\n",
      "169 winrate= 0.42 0.15 0.51 0.09\n",
      "ARENA!!!  0.42 0.16 0.42000000000000004\n",
      "170 winrate= 0.42 0.16 0.56 0.06\n",
      "171 winrate= 0.42 0.16 0.65 0.06\n",
      "172 winrate= 0.42 0.16 0.6 0.03\n",
      "173 winrate= 0.42 0.16 0.51 0.08\n",
      "174 winrate= 0.42 0.16 0.53 0.1\n",
      "ARENA!!!  0.44 0.13 0.43000000000000005\n",
      "upgrade 0.44 0.13 0.43000000000000005\n",
      "175 winrate= 0.44 0.13 0.47 0.11\n",
      "176 winrate= 0.44 0.13 0.5 0.08\n",
      "177 winrate= 0.44 0.13 0.54 0.1\n",
      "178 winrate= 0.44 0.13 0.58 0.05\n",
      "179 winrate= 0.44 0.13 0.54 0.06\n",
      "ARENA!!!  0.49 0.13 0.38\n",
      "upgrade 0.49 0.13 0.38\n",
      "180 winrate= 0.49 0.13 0.59 0.1\n",
      "181 winrate= 0.49 0.13 0.68 0.04\n",
      "182 winrate= 0.49 0.13 0.49 0.06\n",
      "183 winrate= 0.49 0.13 0.46 0.05\n",
      "184 winrate= 0.49 0.13 0.42 0.11\n",
      "ARENA!!!  0.44 0.1 0.4600000000000001\n",
      "185 winrate= 0.44 0.1 0.61 0.06\n",
      "186 winrate= 0.44 0.1 0.49 0.1\n",
      "187 winrate= 0.44 0.1 0.57 0.12\n",
      "188 winrate= 0.44 0.1 0.59 0.07\n",
      "189 winrate= 0.44 0.1 0.48 0.03\n",
      "ARENA!!!  0.51 0.12 0.37\n",
      "upgrade 0.51 0.12 0.37\n",
      "190 winrate= 0.51 0.12 0.63 0.08\n",
      "191 winrate= 0.51 0.12 0.5 0.1\n",
      "192 winrate= 0.51 0.12 0.59 0.12\n",
      "193 winrate= 0.51 0.12 0.62 0.03\n",
      "194 winrate= 0.51 0.12 0.43 0.07\n",
      "ARENA!!!  0.4 0.14 0.45999999999999996\n",
      "195 winrate= 0.4 0.14 0.54 0.07\n",
      "196 winrate= 0.4 0.14 0.53 0.07\n",
      "197 winrate= 0.4 0.14 0.44 0.07\n",
      "198 winrate= 0.4 0.14 0.61 0.04\n",
      "199 winrate= 0.4 0.14 0.53 0.04\n",
      "ARENA!!!  0.44 0.12 0.44000000000000006\n",
      "200 winrate= 0.44 0.12 0.51 0.06\n",
      "201 winrate= 0.44 0.12 0.55 0.09\n",
      "202 winrate= 0.44 0.12 0.54 0.07\n",
      "203 winrate= 0.44 0.12 0.65 0.1\n",
      "204 winrate= 0.44 0.12 0.54 0.13\n",
      "ARENA!!!  0.47 0.11 0.42000000000000004\n",
      "upgrade 0.47 0.11 0.42000000000000004\n",
      "205 winrate= 0.47 0.11 0.49 0.06\n",
      "206 winrate= 0.47 0.11 0.5 0.11\n",
      "207 winrate= 0.47 0.11 0.55 0.08\n",
      "208 winrate= 0.47 0.11 0.64 0.06\n",
      "209 winrate= 0.47 0.11 0.56 0.09\n",
      "ARENA!!!  0.42 0.15 0.43000000000000005\n",
      "210 winrate= 0.42 0.15 0.56 0.06\n",
      "211 winrate= 0.42 0.15 0.49 0.1\n",
      "212 winrate= 0.42 0.15 0.53 0.07\n",
      "213 winrate= 0.42 0.15 0.57 0.03\n",
      "214 winrate= 0.42 0.15 0.53 0.09\n",
      "ARENA!!!  0.45 0.09 0.4600000000000001\n",
      "215 winrate= 0.45 0.09 0.52 0.06\n",
      "216 winrate= 0.45 0.09 0.55 0.08\n",
      "217 winrate= 0.45 0.09 0.47 0.11\n",
      "218 winrate= 0.45 0.09 0.51 0.12\n",
      "219 winrate= 0.45 0.09 0.51 0.05\n",
      "ARENA!!!  0.42 0.12 0.4600000000000001\n",
      "220 winrate= 0.42 0.12 0.55 0.11\n",
      "221 winrate= 0.42 0.12 0.59 0.09\n",
      "222 winrate= 0.42 0.12 0.5 0.07\n",
      "223 winrate= 0.42 0.12 0.53 0.09\n",
      "224 winrate= 0.42 0.12 0.54 0.11\n",
      "ARENA!!!  0.41 0.16 0.43000000000000005\n",
      "225 winrate= 0.41 0.16 0.56 0.03\n",
      "226 winrate= 0.41 0.16 0.46 0.15\n",
      "227 winrate= 0.41 0.16 0.54 0.09\n",
      "228 winrate= 0.41 0.16 0.52 0.1\n",
      "229 winrate= 0.41 0.16 0.49 0.04\n",
      "ARENA!!!  0.41 0.12 0.4700000000000001\n",
      "230 winrate= 0.41 0.12 0.54 0.07\n",
      "231 winrate= 0.41 0.12 0.43 0.13\n",
      "232 winrate= 0.41 0.12 0.46 0.13\n",
      "233 winrate= 0.41 0.12 0.56 0.04\n",
      "234 winrate= 0.41 0.12 0.58 0.1\n",
      "ARENA!!!  0.41 0.12 0.4700000000000001\n",
      "235 winrate= 0.41 0.12 0.48 0.06\n",
      "236 winrate= 0.41 0.12 0.6 0.03\n",
      "237 winrate= 0.41 0.12 0.59 0.05\n",
      "238 winrate= 0.41 0.12 0.53 0.06\n",
      "239 winrate= 0.41 0.12 0.58 0.07\n",
      "ARENA!!!  0.41 0.15 0.44000000000000006\n",
      "240 winrate= 0.41 0.15 0.55 0.03\n",
      "241 winrate= 0.41 0.15 0.52 0.07\n",
      "242 winrate= 0.41 0.15 0.6 0.06\n",
      "243 winrate= 0.41 0.15 0.57 0.07\n",
      "244 winrate= 0.41 0.15 0.53 0.04\n",
      "ARENA!!!  0.44 0.16 0.4\n",
      "upgrade 0.44 0.16 0.4\n",
      "245 winrate= 0.44 0.16 0.53 0.09\n",
      "246 winrate= 0.44 0.16 0.45 0.09\n",
      "247 winrate= 0.44 0.16 0.53 0.09\n",
      "248 winrate= 0.44 0.16 0.51 0.05\n",
      "249 winrate= 0.44 0.16 0.58 0.05\n",
      "ARENA!!!  0.44 0.09 0.4700000000000001\n",
      "250 winrate= 0.44 0.09 0.43 0.08\n",
      "251 winrate= 0.44 0.09 0.59 0.09\n",
      "252 winrate= 0.44 0.09 0.51 0.08\n",
      "253 winrate= 0.44 0.09 0.58 0.09\n",
      "254 winrate= 0.44 0.09 0.57 0.11\n",
      "ARENA!!!  0.48 0.11 0.41000000000000003\n",
      "upgrade 0.48 0.11 0.41000000000000003\n",
      "255 winrate= 0.48 0.11 0.61 0.13\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-0b41e6b3c014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrew_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_mark\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrew_sign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-203-901b55055f08>\u001b[0m in \u001b[0;36mMCTS\u001b[0;34m(root_node, max_depth, n_times, policy, model, alpha_UCB)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnchildren\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-ad400ebeaa53>\u001b[0m in \u001b[0;36mcreate_child\u001b[0;34m(self, ChildType, policy, model)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnot_available_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaken_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_available_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# print(not_available_actions, self.parent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaken_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-4bc311ed4587>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, not_available_actions)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs2testorobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_available_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_available_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-4bc311ed4587>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, not_available_actions)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "rbuff = ReplayBuffer(nitems=3, max_len=150)\n",
    "bsize = 128\n",
    "wll.new()\n",
    "writer = wll.writer\n",
    "opt = torch.optim.Adam(list(tree_policy.parameters())+list(value_function.parameters()), lr=5e-3)\n",
    "import copy\n",
    "best_tree_policy = copy.deepcopy(tree_policy)\n",
    "best_opt = copy.deepcopy(opt)\n",
    "best_vfunc = copy.deepcopy(value_function)\n",
    "\n",
    "for game in range(10000):\n",
    "    game_step = 0\n",
    "    done = False\n",
    "    tmp_buff = []\n",
    "    while not done:\n",
    "        game_step += 1\n",
    "        rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "        root = Node(obs, 0, reward_sign=rew_sign)\n",
    "        dic = MCTS(root, 10, 100, tree_policy, model, 100)\n",
    "        dic = np.array(dic)\n",
    "\n",
    "        tdic = torch.tensor([[-dic]])\n",
    "        monte_probs = torch.softmax(tdic, dim=-1).detach()\n",
    "        tensor_obs = tree_policy.obs2testorobs(obs).unsqueeze(0)\n",
    "        tmp_buff.append([tensor_obs, monte_probs, rew_sign])\n",
    "        \n",
    "        move = np.argmin(dic)\n",
    "        obs, r, done, _ = test_env.step(move)\n",
    "\n",
    "    for elements in tmp_buff:\n",
    "        rbuff.add(elements[0], elements[1], elements[2]*torch.tensor([[[r]]]).float())\n",
    "\n",
    "    if len(rbuff) > bsize:\n",
    "        for opt_step in range(4):\n",
    "            tensor_obs, monte_probs, game_finish = rbuff.get(bsize)\n",
    "            policy_probs = tree_policy(tensor_obs)\n",
    "            loss_policy = -(monte_probs*torch.log(policy_probs+1e-8)).mean()\n",
    "            loss_value  = ((value_function(tensor_obs)-game_finish)**2).mean()\n",
    "            loss = loss_policy + loss_value\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        writer.add_scalar('loss/loss', loss.item(), game)\n",
    "        writer.add_scalar('loss/policy', loss_policy.item(), game)\n",
    "        writer.add_scalar('loss/vfunc', loss_value.item(), game)\n",
    "    else:\n",
    "        print(len(rbuff))\n",
    "    if game % 5 == 0:\n",
    "        winrate, drawrate, loserate = eval_winrate(tree_policy, best_tree_policy, test_env, n_games=100)\n",
    "        print(\"ARENA!!! \", winrate, drawrate, loserate)\n",
    "        if winrate > loserate:\n",
    "            best_tree_policy = copy.deepcopy(tree_policy)\n",
    "            best_opt = copy.deepcopy(opt)\n",
    "            best_vfunc = copy.deepcopy(value_function)\n",
    "            print(\"upgrade\", winrate, drawrate, loserate)\n",
    "        else:\n",
    "            tree_policy = copy.deepcopy(best_tree_policy)\n",
    "            opt = copy.deepcopy(best_opt)\n",
    "            value_function = copy.deepcopy(best_vfunc)\n",
    "\n",
    "        \n",
    "    winrate2, drawrate2, _ = eval_winrate(tree_policy, rollout_policy, test_env, n_games=100)\n",
    "    writer.add_scalar('winrate/winrate', winrate2, game)\n",
    "    writer.add_scalar('winrate/drawrate', drawrate2, game)\n",
    "    print(game, \"winrate=\", winrate, drawrate, winrate2, drawrate2)\n",
    "\n",
    "    obs = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}