{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd95df06ad6614d8ea31ce2c635a29972",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_tictactoe.env import TicTacToeEnv, agent_by_mark, next_mark\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from myrl.buffers import ReplayBuffer\n",
    "from myrl.utils import ExperimentWriter\n",
    "# from myrl.value_functions import \n",
    "\n",
    "env = TicTacToeEnv()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, obs, render=False):\n",
    "        return env.action_space.sample()\n",
    "    def rollout(self, obs, model, render=False):\n",
    "        d = False\n",
    "        rsum = 0\n",
    "        while not d:\n",
    "            obs, r, d, _ = model.step(obs, self.act(obs))\n",
    "            rsum += r\n",
    "        return rsum\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def step(self, obs, action):\n",
    "        self._set_env(obs)\n",
    "        return self.env.step(action)\n",
    "    def available_actions(self, obs):\n",
    "        self._set_env(obs)\n",
    "        return self.env.available_actions()\n",
    "    def not_available_actions(self, obs):\n",
    "        return self._list_cut(list(range(self.get_num_actions())), self.available_actions(obs))\n",
    "    def _list_cut(self, l1, l2):\n",
    "        toret = []\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret\n",
    "    def _set_env(self, obs):\n",
    "        self.env.board = list(obs[0])\n",
    "        self.env.mark  = obs[1] \n",
    "        self.done = False\n",
    "    def get_num_actions(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "class TreePolicy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def act(self, obs, available_actions):\n",
    "        import random\n",
    "        return random.choice(available_actions)\n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        return [1/len(available_actions) for i in range(len(available_actions))]\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, net_arch, middle_activation=F.relu, last_activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.middle_activation = middle_activation\n",
    "        self.last_activation = last_activation\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = self.middle_activation(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        h = self.last_activation(h)\n",
    "        return h\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, net_arch, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, h):\n",
    "        h = torch.tensor(h, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = F.relu(lay(h))\n",
    "        h = self.layers[-1](h)\n",
    "        return h\n",
    "\n",
    "class NNTreePolicy(nn.Module):\n",
    "    def __init__(self, net_arch, backbone, temperature=1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.temperature = temperature\n",
    "        self.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(net_arch[:-1], net_arch[1:])])\n",
    "    def forward(self, x, not_available_actions=None):\n",
    "        h = torch.tensor(x, dtype=torch.float)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        h = self.backbone(h)\n",
    "        for lay in self.layers[:-1]:\n",
    "            h = F.relu(lay(h))\n",
    "        h = self.layers[-1](h)/self.temperature\n",
    "        if not_available_actions is not None and len(not_available_actions)>0:\n",
    "            not_available_actions = torch.tensor(not_available_actions)\n",
    "            try:\n",
    "                h[0, not_available_actions] = float('-inf')\n",
    "            except:\n",
    "                print(not_available_actions)\n",
    "                print(h)\n",
    "                raise\n",
    "        h = torch.softmax(h, dim=1)\n",
    "        return h\n",
    "    def act(self, obs, not_available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs, not_available_actions=not_available_actions)\n",
    "        action = np.random.choice(range(len(h[0])), p=h.detach().squeeze(0).numpy())\n",
    "        return action      \n",
    "    def get_action_probs(self, obs, available_actions):\n",
    "        obs = self.obs2testorobs(obs)\n",
    "        h = self.forward(obs)\n",
    "        return h.tolist()[0]\n",
    "    def obs2testorobs(self, obs):\n",
    "        l2 = [1] if obs[1]=='O' else [-1]\n",
    "        obs = torch.tensor([list(obs[0])+l2])\n",
    "        obs[obs==2] = -1\n",
    "        return obs\n",
    "\n",
    "rollout_policy = RolloutPolicy(env)\n",
    "model = Model(TicTacToeEnv())\n",
    "backbone = Backbone([10, 16])\n",
    "value_function = ValueFunction([16, 4, 1], backbone=backbone)\n",
    "tree_policy = NNTreePolicy([16, 9, 9], backbone=backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, obs, reward, change_child_rew_sign=True, reward_sign=1, done=False, parent=None):\n",
    "        self.n = 0\n",
    "        self.cumulative_reward = 0#reward\n",
    "        self.parent = parent\n",
    "        self.action2child = {}\n",
    "        self.nchildren = 0\n",
    "        self.taken_actions = []\n",
    "        self.obs = obs\n",
    "        self.done = done\n",
    "        self.reward = reward\n",
    "        self.reward_sign = reward_sign\n",
    "        self.change_child_rew_sign = change_child_rew_sign\n",
    "        self.nzeros = 0\n",
    "        self.nones = 0\n",
    "        self.nmones = 0\n",
    "    def get_q(self):\n",
    "        return self.cumulative_reward/(self.n)\n",
    "\n",
    "    def backpropagate(self, r, gamma=1):\n",
    "        self.n += 1\n",
    "        self.cumulative_reward += r*gamma*self.reward_sign\n",
    "        self.nzeros += 1 if r==0 else 0\n",
    "        self.nones += 1 if r*self.reward_sign==1  else 0\n",
    "        self.nmones+= 1 if r*self.reward_sign==-1 else 0\n",
    "        if not( -1 <= self.get_q() <= 1 ) and 0:\n",
    "            print(self.__dict__)\n",
    "        if self.parent is None:\n",
    "            return \n",
    "        self.parent.backpropagate(r, gamma=gamma)\n",
    "    def print_parents(self):\n",
    "        if self.parent is None:\n",
    "            return\n",
    "        self.parent.print_parents()\n",
    "\n",
    "    def create_child(self, ChildType, policy, model):\n",
    "        action = policy.act(self.obs, self.taken_actions)\n",
    "        self.taken_actions.append(action)\n",
    "        obs, reward, done, info = model.step(self.obs, action)\n",
    "        reward_sign= -self.reward_sign if self.change_child_rew_sign else self.reward_sign\n",
    "        child = ChildType(obs, reward, done=done, reward_sign=reward_sign, parent=self, change_child_rew_sign=self.change_child_rew_sign)\n",
    "        self.action2child[action] = child\n",
    "        self.nchildren += 1\n",
    "        return child, action\n",
    "\n",
    "    def _list_cut(self, l1, l2):\n",
    "        toret = []\n",
    "        for a1 in l1:\n",
    "            if a1 not in l2:\n",
    "                toret.append(a1)\n",
    "        return toret\n",
    "\n",
    "    def rollout(self, rollout_policy, model, render=False):\n",
    "        if self.done:\n",
    "            return self.reward\n",
    "        return rollout_policy.rollout(self.obs, model, render=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(root_node, policy, model, alpha=1):\n",
    "    scores = [0 for i in range(model.get_num_actions())]\n",
    "    all_actions = list(range(model.get_num_actions()))\n",
    "    probs = policy.get_action_probs(root_node.obs, all_actions)\n",
    "    minscore, a_minscore = float('inf'), -1\n",
    "    for a in root_node.action2child.keys():\n",
    "        child = root_node.action2child[a]\n",
    "        u = -probs[a]/(1+child.n)\n",
    "        q = child.get_q()\n",
    "        score = q + alpha*u\n",
    "        scores.append(score)\n",
    "        if score < minscore:\n",
    "            minscore = score\n",
    "            a_minscore = a\n",
    "    return scores, (a_minscore, minscore)\n",
    "\n",
    "def MCTS(root_node, max_depth, n_times, policy, model, alpha_UCB=1):\n",
    "    current_node  = root_node\n",
    "    current_depth = 0\n",
    "    n_times_done  = 0\n",
    "\n",
    "    while n_times_done != n_times:\n",
    "        if current_depth == max_depth or current_node.done:\n",
    "            reward = current_node.rollout(rollout_policy, model)\n",
    "            current_node.backpropagate(reward, gamma=0.99)\n",
    "            current_node = root_node\n",
    "            n_times_done += 1\n",
    "            current_depth = 0\n",
    "            model.env.done = False\n",
    "        elif current_node.nchildren < len(model.available_actions(current_node.obs)):\n",
    "            child, action = current_node.create_child(Node, policy, model) \n",
    "            current_node = child\n",
    "            current_depth += 1\n",
    "        else:\n",
    "            scores, (a_minscore, minscore) = UCB(current_node, policy, model, alpha_UCB)\n",
    "            current_node = current_node.action2child[a_minscore]\n",
    "            current_depth += 1\n",
    "\n",
    "    visits = []\n",
    "    for a in range(model.get_num_actions()):\n",
    "        if a in root_node.action2child:\n",
    "            visits.append(root_node.action2child[a].get_q())\n",
    "        else:\n",
    "            visits.append(float('inf'))\n",
    "    return visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_winrate(totest, bench, env, n_games=100):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    rsum = 0\n",
    "    for igame in range(n_games):\n",
    "        done, reward = False, 0\n",
    "        obs = env.reset()\n",
    "        curr_policy = totest if igame<=n_games//2 else bench\n",
    "        rew2count = 1 if igame<=n_games//2 else -1\n",
    "        while not done:\n",
    "            action = curr_policy.act(obs, model.not_available_actions(obs))\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            rsum += r\n",
    "            curr_policy = totest if curr_policy==bench else bench\n",
    "            wins += 1 if r==rew2count else 0\n",
    "        draws += 1 if r==0 else 0\n",
    "    winrate = wins/n_games\n",
    "    drawrate = draws/n_games\n",
    "    return winrate, drawrate, (1-winrate-drawrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      "move probs=  tensor([[0.1352, 0.0666, 0.1341],\n",
      "        [0.1137, 0.1088, 0.1242],\n",
      "        [0.1242, 0.0795, 0.1137]], dtype=torch.float64)\n",
      "0 O\n",
      "[[-0.28285714  0.42428571 -0.275     ]\n",
      " [-0.11       -0.066      -0.198     ]\n",
      " [-0.198       0.2475     -0.11      ]]\n",
      "  O| | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.1996, 0.0742, 0.1354],\n",
      "        [0.0000, 0.1177, 0.1904],\n",
      "        [0.1120, 0.0966, 0.0742]], dtype=torch.float64)\n",
      "0 X\n",
      "[[-0.33        0.66        0.05823529]\n",
      " [        inf  0.198      -0.28285714]\n",
      " [ 0.2475      0.396       0.66      ]]\n",
      "  X| | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.1184, 0.1228],\n",
      "        [0.1437, 0.1115, 0.1417],\n",
      "        [0.1070, 0.1254, 0.1295]], dtype=torch.float64)\n",
      "3 O\n",
      "[[       inf 0.27       0.23294118]\n",
      " [0.07615385 0.33       0.09      ]\n",
      " [0.37125    0.21214286 0.18      ]]\n",
      "  X| | \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.1692, 0.2131, 0.1501],\n",
      "        [0.0000, 0.1341, 0.0528],\n",
      "        [0.1273, 0.0000, 0.1534]], dtype=torch.float64)\n",
      "1 X\n",
      "[[-0.17470588 -0.405      -0.055     ]\n",
      " [        inf  0.05823529  0.99      ]\n",
      " [ 0.11               inf -0.07615385]]\n",
      "  X|X| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.2649, 0.1235, 0.0995],\n",
      "        [0.1835, 0.0000, 0.1506],\n",
      "        [0.1780, 0.0000, 0.0000]], dtype=torch.float64)\n",
      "0 O\n",
      "[[-0.16902439  0.594       0.81      ]\n",
      " [ 0.198              inf  0.396     ]\n",
      " [ 0.22846154         inf         inf]]\n",
      "  O|X| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.2642, 0.0000, 0.0000],\n",
      "        [0.1507, 0.1585, 0.0734],\n",
      "        [0.1649, 0.1884, 0.0000]], dtype=torch.float64)\n",
      "0 X\n",
      "[[-0.29117647         inf         inf]\n",
      " [ 0.27        0.22        0.99      ]\n",
      " [ 0.18        0.04714286         inf]]\n",
      "  X|X| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.2417, 0.1955, 0.0886],\n",
      "        [0.1955, 0.0727, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2060]], dtype=torch.float64)\n",
      "0 O\n",
      "[[-2.12142857e-01  1.23358114e-17  7.92000000e-01]\n",
      " [ 0.00000000e+00  9.90000000e-01             inf]\n",
      " [            inf             inf -5.21052632e-02]]\n",
      "  O|X| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.0000, 0.1239],\n",
      "        [0.1680, 0.0000, 0.0785],\n",
      "        [0.2591, 0.2187, 0.1518]], dtype=torch.float64)\n",
      "6 X\n",
      "[[        inf         inf  0.53307692]\n",
      " [ 0.22846154         inf  0.99      ]\n",
      " [-0.20482759 -0.03535714  0.33      ]]\n",
      "  O|X| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.2365, 0.1713],\n",
      "        [0.0000, 0.2197, 0.1882],\n",
      "        [0.0000, 0.1844, 0.0000]], dtype=torch.float64)\n",
      "1 O\n",
      "[[        inf -0.38076923 -0.05823529]\n",
      " [        inf -0.30724138 -0.15230769]\n",
      " [        inf -0.132              inf]]\n",
      "  O|O| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.2716, 0.0000, 0.0000],\n",
      "        [0.1376, 0.2177, 0.0000],\n",
      "        [0.1872, 0.1859, 0.0000]], dtype=torch.float64)\n",
      "0 X\n",
      "[[0.09              inf        inf]\n",
      " [0.77       0.31114286        inf]\n",
      " [0.462      0.46894737        inf]]\n",
      "  X|O| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.3370, 0.1484, 0.1521],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.1870, 0.1755, 0.0000]], dtype=torch.float64)\n",
      "0 O\n",
      "[[-0.42428571  0.396       0.37125   ]\n",
      " [        inf         inf         inf]\n",
      " [ 0.165       0.22846154         inf]]\n",
      "  O|O| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.2782, 0.1973, 0.0000],\n",
      "        [0.2165, 0.1644, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1436]], dtype=torch.float64)\n",
      "0 X\n",
      "[[-0.0309375   0.31263158         inf]\n",
      " [ 0.22        0.495              inf]\n",
      " [        inf         inf  0.63      ]]\n",
      "  X|O| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.2485, 0.1348, 0.0000],\n",
      "        [0.2099, 0.0000, 0.2211],\n",
      "        [0.0000, 0.0000, 0.1857]], dtype=torch.float64)\n",
      "0 O\n",
      "[[-1.16470588e-01  4.95000000e-01             inf]\n",
      " [ 5.21052632e-02             inf  1.23358114e-17]\n",
      " [            inf             inf  1.74705882e-01]]\n",
      "  O|O| \n",
      "  -----\n",
      "  O| | \n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.1991, 0.2121, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2499],\n",
      "        [0.0000, 0.2277, 0.1112]], dtype=torch.float64)\n",
      "5 X\n",
      "[[0.40764706 0.34434783        inf]\n",
      " [       inf        inf 0.18      ]\n",
      " [       inf 0.27310345 0.99      ]]\n",
      "  O|O| \n",
      "  -----\n",
      "  O| |X\n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n",
      "move probs=  tensor([[0.0000, 0.0000, 0.3193],\n",
      "        [0.0000, 0.2583, 0.0000],\n",
      "        [0.3193, 0.0000, 0.1030]], dtype=torch.float64)\n",
      "2 O\n",
      "[[        inf         inf -0.99      ]\n",
      " [        inf -0.77785714         inf]\n",
      " [-0.99               inf  0.14142857]]\n",
      "  O|O|O\n",
      "  -----\n",
      "  O| |X\n",
      "  -----\n",
      "  X| | \n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "# rbuff = ReplayBuffer(200)\n",
    "random_policy = TreePolicy()\n",
    "\n",
    "while not done:\n",
    "    rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "    root = Node(obs, 0, reward_sign=rew_sign)\n",
    "    dic = MCTS(root, 10, 100, tree_policy, model, 100)\n",
    "    dic = np.array(dic)\n",
    "    tdic = torch.tensor([-dic])\n",
    "    print(\"move probs= \", torch.softmax(tdic, dim=1).view(3, 3))\n",
    "    move = np.argmin(dic)\n",
    "    print(move, test_env.mark)\n",
    "    print(dic.reshape(3, 3))\n",
    "    obs, r, done, _ = test_env.step(move)\n",
    "    test_env.render()\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = copy.deepcopy(tree_policy)\n",
    "wll = ExperimentWriter('tb/alpha_tictacte_zero_valuef_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([1, 8], [6, 4], ((1, 0, 2, 2, 2, 1, 2, 1, 0), 'O'))"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "root._list_cut([1, 2, 3, 5], [2, 3, 4])\n",
    "model.available_actions(obs), root.taken_actions, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "  -----\n",
      "   | | \n",
      "\n",
      "0 winrate= 0.45 0.02\n",
      "1 winrate= 0.58 0.01\n",
      "2 winrate= 0.51 0.0\n",
      "3 winrate= 0.51 0.01\n",
      "4 winrate= 0.47 0.05\n",
      "5 winrate= 0.55 0.01\n",
      "6 winrate= 0.48 0.0\n",
      "7 winrate= 0.55 0.02\n",
      "8 winrate= 0.52 0.0\n",
      "9 winrate= 0.46 0.01\n",
      "10 winrate= 0.5 0.0\n",
      "11 winrate= 0.39 0.01\n",
      "12 winrate= 0.51 0.0\n",
      "13 winrate= 0.53 0.0\n",
      "14 winrate= 0.55 0.01\n",
      "15 winrate= 0.48 0.0\n",
      "16 winrate= 0.48 0.0\n",
      "17 winrate= 0.48 0.02\n",
      "18 winrate= 0.58 0.02\n",
      "19 winrate= 0.46 0.0\n",
      "20 winrate= 0.5 0.02\n",
      "21 winrate= 0.56 0.01\n",
      "22 winrate= 0.54 0.02\n",
      "23 winrate= 0.57 0.01\n",
      "24 winrate= 0.49 0.03\n",
      "25 winrate= 0.55 0.01\n",
      "26 winrate= 0.52 0.02\n",
      "27 winrate= 0.54 0.02\n",
      "28 winrate= 0.56 0.0\n",
      "29 winrate= 0.53 0.01\n",
      "30 winrate= 0.54 0.0\n",
      "31 winrate= 0.5 0.0\n",
      "32 winrate= 0.4 0.02\n",
      "33 winrate= 0.5 0.01\n",
      "34 winrate= 0.55 0.02\n",
      "35 winrate= 0.5 0.0\n",
      "36 winrate= 0.44 0.02\n",
      "37 winrate= 0.53 0.0\n",
      "38 winrate= 0.49 0.04\n",
      "39 winrate= 0.51 0.01\n",
      "40 winrate= 0.49 0.04\n",
      "41 winrate= 0.47 0.01\n",
      "42 winrate= 0.42 0.01\n",
      "43 winrate= 0.41 0.02\n",
      "44 winrate= 0.48 0.02\n",
      "45 winrate= 0.53 0.02\n",
      "46 winrate= 0.43 0.0\n",
      "47 winrate= 0.43 0.02\n",
      "48 winrate= 0.51 0.0\n",
      "49 winrate= 0.51 0.01\n",
      "50 winrate= 0.5 0.02\n",
      "51 winrate= 0.55 0.0\n",
      "52 winrate= 0.44 0.02\n",
      "53 winrate= 0.56 0.02\n",
      "54 winrate= 0.55 0.01\n",
      "55 winrate= 0.52 0.01\n",
      "56 winrate= 0.48 0.01\n",
      "57 winrate= 0.59 0.01\n",
      "58 winrate= 0.54 0.02\n",
      "59 winrate= 0.55 0.0\n",
      "60 winrate= 0.53 0.0\n",
      "61 winrate= 0.57 0.01\n",
      "62 winrate= 0.51 0.01\n",
      "63 winrate= 0.51 0.0\n",
      "64 winrate= 0.53 0.02\n",
      "65 winrate= 0.53 0.0\n",
      "66 winrate= 0.4 0.02\n",
      "67 winrate= 0.6 0.0\n",
      "68 winrate= 0.47 0.05\n",
      "69 winrate= 0.47 0.0\n",
      "70 winrate= 0.53 0.01\n",
      "71 winrate= 0.45 0.01\n",
      "72 winrate= 0.51 0.0\n",
      "73 winrate= 0.52 0.0\n",
      "74 winrate= 0.46 0.0\n",
      "75 winrate= 0.51 0.02\n",
      "76 winrate= 0.54 0.01\n",
      "77 winrate= 0.5 0.01\n",
      "78 winrate= 0.47 0.01\n",
      "79 winrate= 0.49 0.01\n",
      "80 winrate= 0.45 0.03\n",
      "81 winrate= 0.54 0.04\n",
      "82 winrate= 0.5 0.0\n",
      "83 winrate= 0.6 0.0\n",
      "84 winrate= 0.49 0.02\n",
      "85 winrate= 0.53 0.01\n",
      "86 winrate= 0.47 0.03\n",
      "87 winrate= 0.6 0.01\n",
      "88 winrate= 0.61 0.02\n",
      "89 winrate= 0.43 0.03\n",
      "90 winrate= 0.42 0.01\n",
      "91 winrate= 0.49 0.03\n",
      "92 winrate= 0.5 0.0\n",
      "93 winrate= 0.54 0.02\n",
      "94 winrate= 0.37 0.02\n",
      "95 winrate= 0.45 0.03\n",
      "96 winrate= 0.48 0.01\n",
      "97 winrate= 0.44 0.01\n",
      "98 winrate= 0.5 0.01\n",
      "99 winrate= 0.42 0.01\n",
      "100 winrate= 0.51 0.01\n",
      "101 winrate= 0.55 0.0\n",
      "102 winrate= 0.45 0.02\n",
      "103 winrate= 0.53 0.02\n",
      "104 winrate= 0.44 0.02\n",
      "105 winrate= 0.46 0.01\n",
      "106 winrate= 0.55 0.01\n",
      "107 winrate= 0.58 0.01\n",
      "108 winrate= 0.49 0.0\n",
      "109 winrate= 0.41 0.01\n",
      "110 winrate= 0.48 0.01\n",
      "111 winrate= 0.49 0.0\n",
      "112 winrate= 0.51 0.01\n",
      "113 winrate= 0.44 0.02\n",
      "114 winrate= 0.51 0.01\n",
      "115 winrate= 0.51 0.0\n",
      "116 winrate= 0.57 0.0\n",
      "117 winrate= 0.42 0.02\n",
      "118 winrate= 0.51 0.0\n",
      "119 winrate= 0.53 0.03\n",
      "120 winrate= 0.45 0.0\n",
      "121 winrate= 0.64 0.01\n",
      "122 winrate= 0.51 0.02\n",
      "123 winrate= 0.57 0.01\n",
      "124 winrate= 0.57 0.01\n",
      "125 winrate= 0.52 0.03\n",
      "126 winrate= 0.5 0.02\n",
      "127 winrate= 0.54 0.04\n",
      "128 winrate= 0.51 0.01\n",
      "129 winrate= 0.55 0.0\n",
      "130 winrate= 0.42 0.03\n",
      "131 winrate= 0.49 0.0\n",
      "132 winrate= 0.5 0.02\n",
      "133 winrate= 0.51 0.01\n",
      "134 winrate= 0.58 0.01\n",
      "135 winrate= 0.51 0.01\n",
      "136 winrate= 0.46 0.03\n",
      "137 winrate= 0.44 0.03\n",
      "138 winrate= 0.44 0.01\n",
      "139 winrate= 0.49 0.01\n",
      "140 winrate= 0.46 0.02\n",
      "141 winrate= 0.51 0.02\n",
      "142 winrate= 0.51 0.0\n",
      "143 winrate= 0.47 0.01\n",
      "144 winrate= 0.55 0.01\n",
      "145 winrate= 0.48 0.03\n",
      "146 winrate= 0.5 0.0\n",
      "147 winrate= 0.61 0.02\n",
      "148 winrate= 0.54 0.03\n",
      "149 winrate= 0.6 0.0\n",
      "150 winrate= 0.45 0.01\n",
      "151 winrate= 0.58 0.02\n",
      "152 winrate= 0.52 0.0\n",
      "153 winrate= 0.51 0.01\n",
      "154 winrate= 0.51 0.0\n",
      "155 winrate= 0.56 0.01\n",
      "156 winrate= 0.46 0.0\n",
      "157 winrate= 0.5 0.01\n",
      "158 winrate= 0.47 0.0\n",
      "159 winrate= 0.47 0.0\n",
      "160 winrate= 0.54 0.01\n",
      "161 winrate= 0.54 0.0\n",
      "162 winrate= 0.54 0.01\n",
      "163 winrate= 0.6 0.02\n",
      "164 winrate= 0.45 0.0\n",
      "165 winrate= 0.49 0.03\n",
      "166 winrate= 0.44 0.0\n",
      "167 winrate= 0.44 0.0\n",
      "168 winrate= 0.52 0.05\n",
      "169 winrate= 0.58 0.01\n",
      "170 winrate= 0.58 0.02\n",
      "171 winrate= 0.53 0.01\n",
      "172 winrate= 0.61 0.01\n",
      "173 winrate= 0.53 0.01\n",
      "174 winrate= 0.51 0.01\n",
      "175 winrate= 0.43 0.01\n",
      "176 winrate= 0.53 0.01\n",
      "177 winrate= 0.48 0.02\n",
      "178 winrate= 0.54 0.0\n",
      "179 winrate= 0.51 0.01\n",
      "180 winrate= 0.65 0.01\n",
      "181 winrate= 0.51 0.02\n",
      "182 winrate= 0.51 0.0\n",
      "183 winrate= 0.55 0.0\n",
      "184 winrate= 0.59 0.0\n",
      "185 winrate= 0.55 0.03\n",
      "186 winrate= 0.55 0.02\n",
      "187 winrate= 0.51 0.02\n",
      "188 winrate= 0.54 0.02\n",
      "189 winrate= 0.52 0.0\n",
      "190 winrate= 0.5 0.02\n",
      "191 winrate= 0.45 0.03\n",
      "192 winrate= 0.53 0.02\n",
      "193 winrate= 0.53 0.03\n",
      "194 winrate= 0.48 0.01\n",
      "195 winrate= 0.48 0.03\n",
      "196 winrate= 0.49 0.01\n",
      "197 winrate= 0.49 0.0\n",
      "198 winrate= 0.5 0.0\n",
      "199 winrate= 0.55 0.02\n",
      "200 winrate= 0.43 0.01\n",
      "201 winrate= 0.48 0.03\n",
      "202 winrate= 0.47 0.0\n",
      "203 winrate= 0.47 0.0\n",
      "204 winrate= 0.52 0.0\n",
      "205 winrate= 0.52 0.01\n",
      "206 winrate= 0.49 0.0\n",
      "207 winrate= 0.52 0.0\n",
      "208 winrate= 0.48 0.0\n",
      "209 winrate= 0.46 0.01\n",
      "210 winrate= 0.49 0.03\n",
      "211 winrate= 0.59 0.01\n",
      "212 winrate= 0.47 0.0\n",
      "213 winrate= 0.43 0.02\n",
      "214 winrate= 0.48 0.02\n",
      "215 winrate= 0.46 0.03\n",
      "216 winrate= 0.58 0.02\n",
      "217 winrate= 0.54 0.01\n",
      "218 winrate= 0.46 0.01\n",
      "219 winrate= 0.6 0.01\n",
      "220 winrate= 0.45 0.03\n",
      "221 winrate= 0.54 0.0\n",
      "222 winrate= 0.51 0.02\n",
      "223 winrate= 0.54 0.0\n",
      "224 winrate= 0.4 0.02\n",
      "225 winrate= 0.54 0.0\n",
      "226 winrate= 0.47 0.02\n",
      "227 winrate= 0.56 0.01\n",
      "228 winrate= 0.43 0.03\n",
      "229 winrate= 0.49 0.04\n",
      "230 winrate= 0.49 0.0\n",
      "231 winrate= 0.42 0.01\n",
      "232 winrate= 0.51 0.01\n",
      "233 winrate= 0.54 0.01\n",
      "234 winrate= 0.63 0.01\n",
      "235 winrate= 0.48 0.0\n",
      "236 winrate= 0.4 0.03\n",
      "237 winrate= 0.55 0.0\n",
      "238 winrate= 0.52 0.01\n",
      "239 winrate= 0.58 0.01\n",
      "240 winrate= 0.58 0.01\n",
      "241 winrate= 0.54 0.01\n",
      "242 winrate= 0.53 0.01\n",
      "243 winrate= 0.51 0.02\n",
      "244 winrate= 0.54 0.03\n",
      "245 winrate= 0.53 0.03\n",
      "246 winrate= 0.52 0.03\n",
      "247 winrate= 0.47 0.01\n",
      "248 winrate= 0.49 0.02\n",
      "249 winrate= 0.49 0.01\n",
      "250 winrate= 0.51 0.01\n",
      "251 winrate= 0.59 0.0\n",
      "252 winrate= 0.57 0.01\n",
      "253 winrate= 0.52 0.01\n",
      "254 winrate= 0.48 0.03\n",
      "255 winrate= 0.47 0.01\n",
      "256 winrate= 0.55 0.01\n",
      "257 winrate= 0.46 0.0\n",
      "258 winrate= 0.46 0.01\n",
      "259 winrate= 0.53 0.01\n",
      "260 winrate= 0.53 0.01\n",
      "261 winrate= 0.51 0.02\n",
      "262 winrate= 0.51 0.0\n",
      "263 winrate= 0.58 0.01\n",
      "264 winrate= 0.59 0.0\n",
      "265 winrate= 0.51 0.02\n",
      "266 winrate= 0.52 0.03\n",
      "267 winrate= 0.53 0.0\n",
      "268 winrate= 0.54 0.02\n",
      "269 winrate= 0.5 0.02\n",
      "270 winrate= 0.56 0.01\n",
      "271 winrate= 0.56 0.0\n",
      "272 winrate= 0.48 0.01\n",
      "273 winrate= 0.44 0.01\n",
      "274 winrate= 0.52 0.03\n",
      "275 winrate= 0.52 0.02\n",
      "276 winrate= 0.59 0.02\n",
      "277 winrate= 0.51 0.02\n",
      "278 winrate= 0.43 0.01\n",
      "279 winrate= 0.5 0.03\n",
      "280 winrate= 0.59 0.04\n",
      "281 winrate= 0.5 0.0\n",
      "282 winrate= 0.45 0.01\n",
      "283 winrate= 0.49 0.04\n",
      "284 winrate= 0.65 0.0\n",
      "285 winrate= 0.47 0.01\n",
      "286 winrate= 0.48 0.02\n",
      "287 winrate= 0.49 0.02\n",
      "288 winrate= 0.55 0.03\n",
      "289 winrate= 0.48 0.02\n",
      "290 winrate= 0.49 0.02\n",
      "291 winrate= 0.46 0.01\n",
      "292 winrate= 0.5 0.03\n",
      "293 winrate= 0.54 0.02\n",
      "294 winrate= 0.5 0.0\n",
      "295 winrate= 0.54 0.0\n",
      "296 winrate= 0.52 0.02\n",
      "297 winrate= 0.47 0.02\n",
      "298 winrate= 0.63 0.01\n",
      "299 winrate= 0.5 0.01\n",
      "300 winrate= 0.57 0.01\n",
      "301 winrate= 0.51 0.01\n",
      "302 winrate= 0.48 0.01\n",
      "303 winrate= 0.49 0.02\n",
      "304 winrate= 0.53 0.0\n",
      "305 winrate= 0.49 0.02\n",
      "306 winrate= 0.56 0.0\n",
      "307 winrate= 0.51 0.01\n",
      "308 winrate= 0.56 0.02\n",
      "309 winrate= 0.5 0.0\n",
      "310 winrate= 0.52 0.01\n",
      "311 winrate= 0.43 0.01\n",
      "312 winrate= 0.47 0.01\n",
      "313 winrate= 0.48 0.0\n",
      "314 winrate= 0.5 0.02\n",
      "315 winrate= 0.49 0.03\n",
      "316 winrate= 0.45 0.0\n",
      "317 winrate= 0.5 0.02\n",
      "318 winrate= 0.55 0.01\n",
      "319 winrate= 0.48 0.04\n",
      "320 winrate= 0.5 0.0\n",
      "321 winrate= 0.57 0.0\n",
      "322 winrate= 0.46 0.0\n",
      "323 winrate= 0.58 0.02\n",
      "324 winrate= 0.53 0.02\n",
      "325 winrate= 0.49 0.02\n",
      "326 winrate= 0.43 0.01\n",
      "327 winrate= 0.45 0.02\n",
      "328 winrate= 0.5 0.03\n",
      "329 winrate= 0.56 0.01\n",
      "330 winrate= 0.49 0.01\n",
      "331 winrate= 0.48 0.0\n",
      "332 winrate= 0.5 0.0\n",
      "333 winrate= 0.51 0.01\n",
      "334 winrate= 0.51 0.02\n",
      "335 winrate= 0.54 0.0\n",
      "336 winrate= 0.49 0.03\n",
      "337 winrate= 0.52 0.06\n",
      "338 winrate= 0.43 0.04\n",
      "339 winrate= 0.47 0.0\n",
      "340 winrate= 0.53 0.02\n",
      "341 winrate= 0.52 0.01\n",
      "342 winrate= 0.46 0.02\n",
      "343 winrate= 0.56 0.02\n",
      "344 winrate= 0.53 0.01\n",
      "345 winrate= 0.44 0.0\n",
      "346 winrate= 0.5 0.0\n",
      "347 winrate= 0.53 0.0\n",
      "348 winrate= 0.58 0.02\n",
      "349 winrate= 0.49 0.03\n",
      "350 winrate= 0.46 0.0\n",
      "351 winrate= 0.55 0.01\n",
      "352 winrate= 0.51 0.02\n",
      "353 winrate= 0.53 0.01\n",
      "354 winrate= 0.5 0.0\n",
      "355 winrate= 0.53 0.0\n",
      "356 winrate= 0.64 0.02\n",
      "357 winrate= 0.55 0.01\n",
      "358 winrate= 0.42 0.01\n",
      "359 winrate= 0.55 0.0\n",
      "360 winrate= 0.61 0.01\n",
      "361 winrate= 0.47 0.02\n",
      "362 winrate= 0.53 0.01\n",
      "363 winrate= 0.47 0.03\n",
      "364 winrate= 0.49 0.0\n",
      "365 winrate= 0.45 0.01\n",
      "366 winrate= 0.45 0.0\n",
      "367 winrate= 0.46 0.0\n",
      "368 winrate= 0.44 0.01\n",
      "369 winrate= 0.49 0.03\n",
      "370 winrate= 0.53 0.02\n",
      "371 winrate= 0.48 0.01\n",
      "372 winrate= 0.5 0.01\n",
      "373 winrate= 0.6 0.0\n",
      "374 winrate= 0.48 0.01\n",
      "375 winrate= 0.43 0.0\n",
      "376 winrate= 0.55 0.03\n",
      "377 winrate= 0.55 0.02\n",
      "378 winrate= 0.51 0.0\n",
      "379 winrate= 0.54 0.02\n",
      "380 winrate= 0.44 0.05\n",
      "381 winrate= 0.58 0.01\n",
      "382 winrate= 0.61 0.0\n",
      "383 winrate= 0.52 0.01\n",
      "384 winrate= 0.43 0.02\n",
      "385 winrate= 0.46 0.04\n",
      "386 winrate= 0.48 0.0\n",
      "387 winrate= 0.49 0.0\n",
      "388 winrate= 0.52 0.02\n",
      "389 winrate= 0.52 0.0\n",
      "390 winrate= 0.46 0.03\n",
      "391 winrate= 0.5 0.0\n",
      "392 winrate= 0.64 0.0\n",
      "393 winrate= 0.57 0.02\n",
      "394 winrate= 0.49 0.01\n",
      "395 winrate= 0.53 0.02\n",
      "396 winrate= 0.46 0.0\n",
      "397 winrate= 0.47 0.02\n",
      "398 winrate= 0.47 0.03\n",
      "399 winrate= 0.49 0.0\n",
      "400 winrate= 0.51 0.02\n",
      "401 winrate= 0.52 0.01\n",
      "402 winrate= 0.38 0.01\n",
      "403 winrate= 0.48 0.0\n",
      "404 winrate= 0.52 0.03\n",
      "405 winrate= 0.53 0.01\n",
      "406 winrate= 0.53 0.03\n",
      "407 winrate= 0.51 0.0\n",
      "408 winrate= 0.5 0.01\n",
      "409 winrate= 0.47 0.01\n",
      "410 winrate= 0.51 0.02\n",
      "411 winrate= 0.48 0.01\n",
      "412 winrate= 0.52 0.03\n",
      "413 winrate= 0.5 0.03\n",
      "414 winrate= 0.52 0.0\n",
      "415 winrate= 0.43 0.04\n",
      "416 winrate= 0.55 0.01\n",
      "417 winrate= 0.48 0.02\n",
      "418 winrate= 0.5 0.01\n",
      "419 winrate= 0.48 0.02\n",
      "420 winrate= 0.58 0.03\n",
      "421 winrate= 0.5 0.03\n",
      "422 winrate= 0.54 0.01\n",
      "423 winrate= 0.45 0.01\n",
      "424 winrate= 0.49 0.03\n",
      "425 winrate= 0.54 0.0\n",
      "426 winrate= 0.54 0.0\n",
      "427 winrate= 0.54 0.02\n",
      "428 winrate= 0.52 0.01\n",
      "429 winrate= 0.52 0.01\n",
      "430 winrate= 0.47 0.01\n",
      "431 winrate= 0.43 0.04\n",
      "432 winrate= 0.6 0.02\n",
      "433 winrate= 0.5 0.02\n",
      "434 winrate= 0.39 0.02\n",
      "435 winrate= 0.39 0.02\n",
      "436 winrate= 0.57 0.01\n",
      "437 winrate= 0.54 0.04\n",
      "438 winrate= 0.5 0.01\n",
      "439 winrate= 0.45 0.01\n",
      "440 winrate= 0.51 0.0\n",
      "441 winrate= 0.54 0.05\n",
      "442 winrate= 0.5 0.0\n",
      "443 winrate= 0.45 0.0\n",
      "444 winrate= 0.46 0.01\n",
      "445 winrate= 0.44 0.04\n",
      "446 winrate= 0.5 0.03\n",
      "447 winrate= 0.5 0.02\n",
      "448 winrate= 0.48 0.03\n",
      "449 winrate= 0.51 0.01\n",
      "450 winrate= 0.54 0.0\n",
      "451 winrate= 0.52 0.03\n",
      "452 winrate= 0.53 0.0\n",
      "453 winrate= 0.52 0.03\n",
      "454 winrate= 0.55 0.0\n",
      "455 winrate= 0.49 0.02\n",
      "456 winrate= 0.44 0.0\n",
      "457 winrate= 0.57 0.0\n",
      "458 winrate= 0.5 0.02\n",
      "459 winrate= 0.49 0.03\n",
      "460 winrate= 0.48 0.03\n",
      "461 winrate= 0.44 0.03\n",
      "462 winrate= 0.47 0.0\n",
      "463 winrate= 0.43 0.03\n",
      "464 winrate= 0.52 0.01\n",
      "465 winrate= 0.53 0.0\n",
      "466 winrate= 0.49 0.0\n",
      "467 winrate= 0.54 0.01\n",
      "468 winrate= 0.42 0.0\n",
      "469 winrate= 0.54 0.01\n",
      "470 winrate= 0.5 0.0\n",
      "471 winrate= 0.52 0.01\n",
      "472 winrate= 0.45 0.0\n",
      "473 winrate= 0.53 0.0\n",
      "474 winrate= 0.48 0.01\n",
      "475 winrate= 0.52 0.02\n",
      "476 winrate= 0.5 0.02\n",
      "477 winrate= 0.44 0.01\n",
      "478 winrate= 0.48 0.02\n",
      "479 winrate= 0.4 0.0\n",
      "480 winrate= 0.6 0.03\n",
      "481 winrate= 0.57 0.01\n",
      "482 winrate= 0.45 0.04\n",
      "483 winrate= 0.56 0.02\n",
      "484 winrate= 0.48 0.05\n",
      "485 winrate= 0.57 0.04\n",
      "486 winrate= 0.53 0.01\n",
      "487 winrate= 0.61 0.0\n",
      "488 winrate= 0.51 0.02\n",
      "489 winrate= 0.51 0.02\n",
      "490 winrate= 0.52 0.02\n",
      "491 winrate= 0.48 0.02\n",
      "492 winrate= 0.54 0.01\n",
      "493 winrate= 0.55 0.04\n",
      "494 winrate= 0.54 0.0\n",
      "495 winrate= 0.5 0.03\n",
      "496 winrate= 0.55 0.03\n",
      "497 winrate= 0.45 0.0\n",
      "498 winrate= 0.54 0.04\n",
      "499 winrate= 0.48 0.0\n",
      "500 winrate= 0.51 0.01\n",
      "501 winrate= 0.53 0.01\n",
      "502 winrate= 0.45 0.01\n",
      "503 winrate= 0.55 0.02\n",
      "504 winrate= 0.49 0.01\n",
      "505 winrate= 0.54 0.01\n",
      "506 winrate= 0.56 0.01\n",
      "507 winrate= 0.51 0.05\n",
      "508 winrate= 0.5 0.01\n",
      "509 winrate= 0.4 0.02\n",
      "510 winrate= 0.47 0.02\n",
      "511 winrate= 0.45 0.02\n",
      "512 winrate= 0.57 0.0\n",
      "513 winrate= 0.43 0.02\n",
      "514 winrate= 0.42 0.01\n",
      "515 winrate= 0.49 0.01\n",
      "516 winrate= 0.44 0.0\n",
      "517 winrate= 0.5 0.03\n",
      "518 winrate= 0.48 0.04\n",
      "519 winrate= 0.53 0.0\n",
      "520 winrate= 0.5 0.01\n",
      "521 winrate= 0.58 0.0\n",
      "522 winrate= 0.58 0.02\n",
      "523 winrate= 0.54 0.02\n",
      "524 winrate= 0.47 0.0\n",
      "525 winrate= 0.52 0.02\n",
      "526 winrate= 0.55 0.01\n",
      "527 winrate= 0.55 0.02\n",
      "528 winrate= 0.5 0.01\n",
      "529 winrate= 0.51 0.0\n",
      "530 winrate= 0.5 0.0\n",
      "531 winrate= 0.4 0.02\n",
      "532 winrate= 0.52 0.02\n",
      "533 winrate= 0.49 0.02\n",
      "534 winrate= 0.5 0.01\n",
      "535 winrate= 0.45 0.01\n",
      "536 winrate= 0.52 0.0\n",
      "537 winrate= 0.49 0.0\n",
      "538 winrate= 0.55 0.0\n",
      "539 winrate= 0.55 0.0\n",
      "540 winrate= 0.5 0.01\n",
      "541 winrate= 0.53 0.01\n",
      "542 winrate= 0.55 0.01\n",
      "543 winrate= 0.51 0.01\n",
      "544 winrate= 0.46 0.02\n",
      "545 winrate= 0.6 0.0\n",
      "546 winrate= 0.51 0.02\n",
      "547 winrate= 0.43 0.02\n",
      "548 winrate= 0.5 0.01\n",
      "549 winrate= 0.48 0.02\n",
      "550 winrate= 0.57 0.0\n",
      "551 winrate= 0.6 0.0\n",
      "552 winrate= 0.51 0.02\n",
      "553 winrate= 0.44 0.0\n",
      "554 winrate= 0.41 0.04\n",
      "555 winrate= 0.5 0.03\n",
      "556 winrate= 0.61 0.03\n",
      "557 winrate= 0.52 0.02\n",
      "558 winrate= 0.48 0.02\n",
      "559 winrate= 0.54 0.01\n",
      "560 winrate= 0.59 0.01\n",
      "561 winrate= 0.47 0.02\n",
      "562 winrate= 0.5 0.01\n",
      "563 winrate= 0.52 0.0\n",
      "564 winrate= 0.53 0.02\n",
      "565 winrate= 0.46 0.02\n",
      "566 winrate= 0.57 0.03\n",
      "567 winrate= 0.51 0.02\n",
      "568 winrate= 0.48 0.03\n",
      "569 winrate= 0.54 0.02\n",
      "570 winrate= 0.51 0.01\n",
      "571 winrate= 0.49 0.01\n",
      "572 winrate= 0.53 0.02\n",
      "573 winrate= 0.49 0.03\n",
      "574 winrate= 0.62 0.03\n",
      "575 winrate= 0.45 0.02\n",
      "576 winrate= 0.57 0.05\n",
      "577 winrate= 0.52 0.0\n",
      "578 winrate= 0.47 0.0\n",
      "579 winrate= 0.47 0.0\n",
      "580 winrate= 0.49 0.0\n",
      "581 winrate= 0.48 0.01\n",
      "582 winrate= 0.51 0.02\n",
      "583 winrate= 0.47 0.02\n",
      "584 winrate= 0.54 0.0\n",
      "585 winrate= 0.56 0.01\n",
      "586 winrate= 0.44 0.03\n",
      "587 winrate= 0.49 0.01\n",
      "588 winrate= 0.49 0.02\n",
      "589 winrate= 0.52 0.01\n",
      "590 winrate= 0.51 0.02\n",
      "591 winrate= 0.38 0.04\n",
      "592 winrate= 0.44 0.02\n",
      "593 winrate= 0.48 0.04\n",
      "594 winrate= 0.52 0.01\n",
      "595 winrate= 0.56 0.01\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b179d71a1358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrew_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_mark\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrew_sign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-7eacc0e929a9>\u001b[0m in \u001b[0;36mMCTS\u001b[0;34m(root_node, max_depth, n_times, policy, model, alpha_UCB)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnchildren\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-01f46d4684c7>\u001b[0m in \u001b[0;36mcreate_child\u001b[0;34m(self, ChildType, policy, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChildType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnon_taken_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaken_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_taken_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaken_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-cd25fa7054d8>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, available_actions)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs2testorobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;31m# print(h.detach().squeeze(0).numpy().shape, len(h))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-cd25fa7054d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;31m# if print:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#     print(h.shape, x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_modules'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m    772\u001b[0m             type(self).__name__, name))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_env = TicTacToeEnv()\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "test_env.render()\n",
    "# rbuff = ReplayBuffer(nitems=3, max_len=150)\n",
    "bsize = 128\n",
    "wll.new()\n",
    "writer = wll.writer\n",
    "# opt = torch.optim.Adam(list(tree_policy.parameters())+list(value_function.parameters()), lr=5e-3)\n",
    "import copy\n",
    "best_tree_policy = copy.deepcopy(tree_policy)\n",
    "best_opt = copy.deepcopy(opt)\n",
    "best_vfunc = copy.deepcopy(value_function)\n",
    "\n",
    "for game in range(10000):\n",
    "    game_step = 0\n",
    "    done = False\n",
    "    tmp_buff = []\n",
    "    while not done:\n",
    "        game_step += 1\n",
    "        rew_sign = 1 if test_env.mark==test_env.start_mark else -1\n",
    "        root = Node(obs, 0, reward_sign=rew_sign)\n",
    "        dic = MCTS(root, 10, 100, tree_policy, model, 100)\n",
    "        dic = np.array(dic)\n",
    "\n",
    "        tdic = torch.tensor([[-dic]])\n",
    "        monte_probs = torch.softmax(tdic, dim=-1).detach()\n",
    "        tensor_obs = tree_policy.obs2testorobs(obs).unsqueeze(0)\n",
    "        # rbuff.add(tensor_obs, monte_probs)\n",
    "        tmp_buff.append([tensor_obs, monte_probs, rew_sign])\n",
    "        \n",
    "        move = np.argmin(dic)\n",
    "        obs, r, done, _ = test_env.step(move)\n",
    "\n",
    "    for elements in tmp_buff:\n",
    "        # print(elements[0].shape, elements[1].shape, torch.tensor([[[r]]]).shape)\n",
    "        rbuff.add(elements[0], elements[1], elements[2]*torch.tensor([[[r]]]).float())\n",
    "\n",
    "    if len(rbuff) > bsize:\n",
    "        for opt_step in range(4):\n",
    "            tensor_obs, monte_probs, game_finish = rbuff.get(bsize)\n",
    "            policy_probs = tree_policy(tensor_obs)\n",
    "            loss_policy = -(monte_probs*torch.log(policy_probs+1e-8)).mean()\n",
    "            loss_value  = ((value_function(tensor_obs)-game_finish)**2).mean()\n",
    "            loss = loss_policy + loss_value\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        writer.add_scalar('loss/loss', loss.item(), game)\n",
    "        writer.add_scalar('loss/policy', loss_policy.item(), game)\n",
    "        writer.add_scalar('loss/vfunc', loss_value.item(), game)\n",
    "        # else:\n",
    "        #     print(len(rbuff))\n",
    "    # winrate, drawrate, loserate = eval_winrate(tree_policy, best_tree_policy, test_env, n_games=100)\n",
    "    # if winrate > 0.55:\n",
    "    #     best_tree_policy = copy.deepcopy(tree_policy)\n",
    "    #     best_opt = copy.deepcopy(opt)\n",
    "    #     best_vfunc = copy.deepcopy(value_function)\n",
    "    #     print(\"upgrade\", winrate, drawrate, loserate)\n",
    "    # else:\n",
    "    #     tree_policy = copy.deepcopy(best_tree_policy)\n",
    "    #     opt = copy.deepcopy(best_opt)\n",
    "    #     value_function = copy.deepcopy(best_vfunc)\n",
    "        \n",
    "    winrate, drawrate, _ = eval_winrate(tree_policy, rollout_policy, test_env, n_games=100)\n",
    "    writer.add_scalar('winrate/winrate', winrate, game)\n",
    "    writer.add_scalar('winrate/drawrate', drawrate, game)\n",
    "    print(game, \"winrate=\", winrate, drawrate)\n",
    "\n",
    "    obs = test_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "deque([tensor([[0.0000, 0.1192, 0.1454, 0.1302, 0.0000, 0.1553, 0.1772, 0.1454, 0.1274]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1542, 0.1768, 0.0627, 0.1909, 0.1364, 0.1687, 0.0000, 0.1104]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1316, 0.1316, 0.2254, 0.0000, 0.3300, 0.0000, 0.0000, 0.1813, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2119, 0.0000, 0.2006, 0.0000, 0.0000, 0.2557, 0.0000, 0.1292, 0.2026]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.2197, 0.0000, 0.2845, 0.0000, 0.1999, 0.2960]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2037, 0.0000, 0.0000, 0.0000, 0.2988, 0.2249, 0.0000, 0.2726, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.2263, 0.1032, 0.0000, 0.0000, 0.0000, 0.1816, 0.4888, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1116, 0.0000, 0.0680, 0.0000, 0.2296, 0.1618, 0.1430, 0.1430, 0.1430]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.1638, 0.0000, 0.2254, 0.2463, 0.2150, 0.0000, 0.1496]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0629, 0.2210, 0.0000, 0.1107, 0.0000, 0.2573, 0.1454, 0.2027]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2038, 0.0000, 0.0000, 0.1561, 0.3494, 0.0000, 0.0000, 0.1767, 0.1139]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2843, 0.0000, 0.1974, 0.0000, 0.0000, 0.2741, 0.0000, 0.2442, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.1531, 0.5731, 0.0791, 0.0000, 0.0000, 0.1946, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1956, 0.0000, 0.0000, 0.0000, 0.2834, 0.1602, 0.1956, 0.0000, 0.1653]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2048, 0.1873, 0.0000, 0.0000, 0.2343, 0.0000, 0.1933, 0.0000, 0.1802]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0962, 0.1934, 0.0000, 0.0000, 0.2021, 0.3039, 0.0000, 0.2045]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1985, 0.0000, 0.0000, 0.2328, 0.0000, 0.2558, 0.1544, 0.1585]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1043, 0.1043, 0.1245, 0.0000, 0.1985, 0.1576, 0.2003, 0.0000, 0.1104]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.3543, 0.2137, 0.0000, 0.0000, 0.2312, 0.0000, 0.0000, 0.2007]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.2455, 0.1530, 0.0000, 0.2824, 0.0000, 0.0000, 0.1753, 0.1438]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.1585, 0.3331, 0.1178, 0.2205, 0.0000, 0.1701]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2822, 0.0000, 0.3018, 0.0000, 0.0000, 0.2364, 0.0000, 0.0000, 0.1796]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2268, 0.4238, 0.0000, 0.3494]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.1731, 0.2474, 0.2017, 0.2310, 0.0000, 0.1467]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.3274, 0.2189, 0.0000, 0.1410, 0.0000, 0.3127, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1287, 0.3739, 0.1742, 0.0000, 0.0000, 0.0000, 0.0000, 0.1873, 0.1360]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0733, 0.5307, 0.2158, 0.0000, 0.0000, 0.1802]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2945, 0.0000, 0.1645, 0.2699, 0.0000, 0.0000, 0.0000, 0.0000, 0.2711]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1783, 0.0000, 0.0000, 0.2034, 0.2118, 0.0000, 0.1548, 0.1087, 0.1431]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.3274, 0.0000, 0.0000, 0.0000, 0.0000, 0.2800, 0.3926]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.1997, 0.2722, 0.0000, 0.2035, 0.0000, 0.3246]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.1516, 0.1270, 0.2418, 0.0000, 0.0000, 0.2504, 0.2292]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.3596, 0.0000, 0.3928, 0.0000, 0.0000, 0.0000, 0.0000, 0.2477]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5294, 0.4706, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.6102, 0.1630, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1163, 0.1913, 0.1663, 0.0000, 0.1127, 0.0000, 0.0850, 0.1882, 0.1401]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2126, 0.1537, 0.1891, 0.0000, 0.0000, 0.1642, 0.1522, 0.1282, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1768, 0.0000, 0.1264, 0.2563, 0.2563, 0.1843, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1845, 0.0000, 0.2127, 0.0000, 0.2469, 0.1831, 0.0000, 0.1727, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.2399, 0.0000, 0.1915, 0.0000, 0.2381, 0.1785, 0.0000, 0.1520]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1654, 0.0000, 0.0000, 0.3476, 0.1394, 0.0000, 0.0000, 0.0000, 0.3476]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0739, 0.1943, 0.0000, 0.1820, 0.1919, 0.0000, 0.1802, 0.0000, 0.1777]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0695, 0.1345, 0.2280, 0.0000, 0.2280, 0.2259, 0.0000, 0.0000, 0.1140]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2849, 0.2095, 0.0000, 0.0000, 0.3102, 0.0000, 0.1954, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.2755, 0.2151, 0.0000, 0.2110, 0.1248, 0.0000, 0.0000, 0.1736]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2619, 0.0000, 0.2302, 0.0000, 0.2382, 0.0000, 0.0000, 0.0000, 0.2697]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1302, 0.1531, 0.0000, 0.2875, 0.0000, 0.0000, 0.1417, 0.2875]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1600, 0.0000, 0.1623, 0.1836, 0.1385, 0.0000, 0.1830, 0.0000, 0.1726]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.2129, 0.0000, 0.0000, 0.2009, 0.2773, 0.3089, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.5200, 0.0000, 0.0000, 0.0000, 0.0000, 0.4800]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.1368, 0.1623, 0.2185, 0.4824, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1062, 0.1742, 0.0000, 0.2423, 0.1662, 0.0000, 0.1496, 0.1614, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2549, 0.2400, 0.2921, 0.2131]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1876, 0.0000, 0.0000, 0.1575, 0.2459, 0.2420, 0.0000, 0.1670]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.5155, 0.4845, 0.0000, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1524, 0.1668, 0.0000, 0.4488, 0.0000, 0.0000, 0.0000, 0.2320]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1696, 0.0000, 0.1587, 0.0000, 0.1672, 0.0000, 0.1856, 0.1811, 0.1378]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.3338, 0.0000, 0.0000, 0.1711, 0.2369, 0.1177, 0.1404, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.4146, 0.2996, 0.0000, 0.0000, 0.2858]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.3773, 0.0000, 0.0000, 0.0000, 0.0000, 0.2640, 0.0000, 0.3587, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.4444, 0.1843, 0.1869, 0.0000, 0.1843, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1535, 0.1691, 0.1216, 0.1127, 0.0000, 0.1528, 0.1528, 0.1376]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1247, 0.1659, 0.0000, 0.1894, 0.0000, 0.1876, 0.0000, 0.1164, 0.2161]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1807, 0.1584, 0.1402, 0.0717, 0.0934, 0.1640, 0.0000, 0.0000, 0.1914]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.1571, 0.0000, 0.1897, 0.1792, 0.0000, 0.0000, 0.1206, 0.1647, 0.1887]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.2550, 0.2176, 0.1762, 0.0000, 0.1649, 0.1863, 0.0000, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.3549, 0.0000, 0.0000, 0.1011, 0.2062, 0.1597, 0.1780, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.0000, 0.3091, 0.1646, 0.2481, 0.1460, 0.0000, 0.1321, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1323, 0.2858, 0.1923, 0.1644, 0.0000, 0.0000, 0.0000, 0.2253]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.2645, 0.3759, 0.0000, 0.0000, 0.0000, 0.1432, 0.0000, 0.2164]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.1724, 0.2957, 0.2409, 0.0000, 0.0000, 0.1455, 0.1455, 0.0000]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.4302, 0.0000, 0.0000, 0.0000, 0.3040, 0.0000, 0.0000, 0.2658]],\n",
       "              dtype=torch.float64),\n",
       "       tensor([[0.0000, 0.4515, 0.0000, 0.0000, 0.1811, 0.0000, 0.3674, 0.0000, 0.0000]],\n",
       "              dtype=torch.float64)])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "rbuff.deqs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(rbuff.add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NNTreePolicy(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=9, out_features=9, bias=True)\n",
       "    (1): Linear(in_features=9, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "tree_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 0 1\n",
    "0 0 0 \n",
    "0 0 0"
   ]
  }
 ]
}